{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9ec090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaeb4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"data/X_train.csv\", encoding='cp949')\n",
    "y_train = pd.read_csv(\"data/y_train.csv\", encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feacfaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>총구매액</th>\n",
       "      <th>최대구매액</th>\n",
       "      <th>환불금액</th>\n",
       "      <th>주구매상품</th>\n",
       "      <th>주구매지점</th>\n",
       "      <th>내점일수</th>\n",
       "      <th>내점당구매건수</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>구매주기</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68282840</td>\n",
       "      <td>11264000</td>\n",
       "      <td>6860000.0</td>\n",
       "      <td>기타</td>\n",
       "      <td>강남점</td>\n",
       "      <td>19</td>\n",
       "      <td>3.894737</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2136000</td>\n",
       "      <td>2136000</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>잠실점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3197000</td>\n",
       "      <td>1639000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>남성 캐주얼</td>\n",
       "      <td>관악점</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16077620</td>\n",
       "      <td>4935000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타</td>\n",
       "      <td>광주점</td>\n",
       "      <td>18</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>29050000</td>\n",
       "      <td>24000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>보석</td>\n",
       "      <td>본  점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>3495</td>\n",
       "      <td>3175200</td>\n",
       "      <td>3042900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>골프</td>\n",
       "      <td>본  점</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>3496</td>\n",
       "      <td>29628600</td>\n",
       "      <td>7200000</td>\n",
       "      <td>6049600.0</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>부산본점</td>\n",
       "      <td>8</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>3497</td>\n",
       "      <td>75000</td>\n",
       "      <td>75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>주방용품</td>\n",
       "      <td>창원점</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>3498</td>\n",
       "      <td>1875000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>화장품</td>\n",
       "      <td>본  점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>3499</td>\n",
       "      <td>263101550</td>\n",
       "      <td>34632000</td>\n",
       "      <td>5973000.0</td>\n",
       "      <td>기타</td>\n",
       "      <td>본  점</td>\n",
       "      <td>38</td>\n",
       "      <td>2.421053</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id       총구매액     최대구매액       환불금액   주구매상품 주구매지점  내점일수   내점당구매건수  \\\n",
       "0           0   68282840  11264000  6860000.0      기타   강남점    19  3.894737   \n",
       "1           1    2136000   2136000   300000.0     스포츠   잠실점     2  1.500000   \n",
       "2           2    3197000   1639000        NaN  남성 캐주얼   관악점     2  2.000000   \n",
       "3           3   16077620   4935000        NaN      기타   광주점    18  2.444444   \n",
       "4           4   29050000  24000000        NaN      보석  본  점     2  1.500000   \n",
       "...       ...        ...       ...        ...     ...   ...   ...       ...   \n",
       "3495     3495    3175200   3042900        NaN      골프  본  점     1  2.000000   \n",
       "3496     3496   29628600   7200000  6049600.0    시티웨어  부산본점     8  1.625000   \n",
       "3497     3497      75000     75000        NaN    주방용품   창원점     1  1.000000   \n",
       "3498     3498    1875000   1000000        NaN     화장품  본  점     2  1.000000   \n",
       "3499     3499  263101550  34632000  5973000.0      기타  본  점    38  2.421053   \n",
       "\n",
       "        주말방문비율  구매주기  \n",
       "0     0.527027    17  \n",
       "1     0.000000     1  \n",
       "2     0.000000     1  \n",
       "3     0.318182    16  \n",
       "4     0.000000    85  \n",
       "...        ...   ...  \n",
       "3495  1.000000     0  \n",
       "3496  0.461538    40  \n",
       "3497  0.000000     0  \n",
       "3498  0.000000    39  \n",
       "3499  0.467391     8  \n",
       "\n",
       "[3500 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47279617",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd23ea8",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec46407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>총구매액</th>\n",
       "      <th>최대구매액</th>\n",
       "      <th>환불금액</th>\n",
       "      <th>주구매상품</th>\n",
       "      <th>주구매지점</th>\n",
       "      <th>내점일수</th>\n",
       "      <th>내점당구매건수</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>구매주기</th>\n",
       "      <th>환불여부</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68282840</td>\n",
       "      <td>11264000</td>\n",
       "      <td>6860000.0</td>\n",
       "      <td>기타</td>\n",
       "      <td>강남점</td>\n",
       "      <td>19</td>\n",
       "      <td>3.894737</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2136000</td>\n",
       "      <td>2136000</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>잠실점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3197000</td>\n",
       "      <td>1639000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>남성 캐주얼</td>\n",
       "      <td>관악점</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>16077620</td>\n",
       "      <td>4935000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타</td>\n",
       "      <td>광주점</td>\n",
       "      <td>18</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>29050000</td>\n",
       "      <td>24000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>보석</td>\n",
       "      <td>본  점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>3495</td>\n",
       "      <td>3175200</td>\n",
       "      <td>3042900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>골프</td>\n",
       "      <td>본  점</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>3496</td>\n",
       "      <td>29628600</td>\n",
       "      <td>7200000</td>\n",
       "      <td>6049600.0</td>\n",
       "      <td>시티웨어</td>\n",
       "      <td>부산본점</td>\n",
       "      <td>8</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>3497</td>\n",
       "      <td>75000</td>\n",
       "      <td>75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>주방용품</td>\n",
       "      <td>창원점</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>3498</td>\n",
       "      <td>1875000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>화장품</td>\n",
       "      <td>본  점</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>3499</td>\n",
       "      <td>263101550</td>\n",
       "      <td>34632000</td>\n",
       "      <td>5973000.0</td>\n",
       "      <td>기타</td>\n",
       "      <td>본  점</td>\n",
       "      <td>38</td>\n",
       "      <td>2.421053</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id       총구매액     최대구매액       환불금액   주구매상품 주구매지점  내점일수   내점당구매건수  \\\n",
       "0           0   68282840  11264000  6860000.0      기타   강남점    19  3.894737   \n",
       "1           1    2136000   2136000   300000.0     스포츠   잠실점     2  1.500000   \n",
       "2           2    3197000   1639000        NaN  남성 캐주얼   관악점     2  2.000000   \n",
       "3           3   16077620   4935000        NaN      기타   광주점    18  2.444444   \n",
       "4           4   29050000  24000000        NaN      보석  본  점     2  1.500000   \n",
       "...       ...        ...       ...        ...     ...   ...   ...       ...   \n",
       "3495     3495    3175200   3042900        NaN      골프  본  점     1  2.000000   \n",
       "3496     3496   29628600   7200000  6049600.0    시티웨어  부산본점     8  1.625000   \n",
       "3497     3497      75000     75000        NaN    주방용품   창원점     1  1.000000   \n",
       "3498     3498    1875000   1000000        NaN     화장품  본  점     2  1.000000   \n",
       "3499     3499  263101550  34632000  5973000.0      기타  본  점    38  2.421053   \n",
       "\n",
       "        주말방문비율  구매주기  환불여부  gender  \n",
       "0     0.527027    17     1       0  \n",
       "1     0.000000     1     1       0  \n",
       "2     0.000000     1     0       1  \n",
       "3     0.318182    16     0       1  \n",
       "4     0.000000    85     0       0  \n",
       "...        ...   ...   ...     ...  \n",
       "3495  1.000000     0     0       1  \n",
       "3496  0.461538    40     1       1  \n",
       "3497  0.000000     0     0       0  \n",
       "3498  0.000000    39     0       0  \n",
       "3499  0.467391     8     1       0  \n",
       "\n",
       "[3500 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    for idx in range(0, X_total):\n",
    "        if int(X_train.loc[idx,['환불금액']]>0) :\n",
    "            X_train.loc[idx, ['환불여부']] = 1\n",
    "        else:\n",
    "            X_train.loc[idx, ['환불여부']] = 0\n",
    "\n",
    "    X_train['gender'] = y_train['gender']\n",
    "\n",
    "    X_train.astype({'환불여부':'int'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96011a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Get Dummies--\n",
      "      가공식품  가구  건강식품  골프  구두  기타  남성 캐주얼  남성 트랜디  남성정장  농산물  ...  차/커피  축산가공  \\\n",
      "0        0   0     0   0   0   1       0       0     0    0  ...     0     0   \n",
      "1        0   0     0   0   0   0       0       0     0    0  ...     0     0   \n",
      "2        0   0     0   0   0   0       1       0     0    0  ...     0     0   \n",
      "3        0   0     0   0   0   1       0       0     0    0  ...     0     0   \n",
      "4        0   0     0   0   0   0       0       0     0    0  ...     0     0   \n",
      "...    ...  ..   ...  ..  ..  ..     ...     ...   ...  ...  ...   ...   ...   \n",
      "3495     0   0     0   1   0   0       0       0     0    0  ...     0     0   \n",
      "3496     0   0     0   0   0   0       0       0     0    0  ...     0     0   \n",
      "3497     0   0     0   0   0   0       0       0     0    0  ...     0     0   \n",
      "3498     0   0     0   0   0   0       0       0     0    0  ...     0     0   \n",
      "3499     0   0     0   0   0   1       0       0     0    0  ...     0     0   \n",
      "\n",
      "      침구/수예  캐주얼  커리어  통신/컴퓨터  트래디셔널  피혁잡화  화장품  cust_id  \n",
      "0         0    0    0       0      0     0    0        0  \n",
      "1         0    0    0       0      0     0    0        1  \n",
      "2         0    0    0       0      0     0    0        2  \n",
      "3         0    0    0       0      0     0    0        3  \n",
      "4         0    0    0       0      0     0    0        4  \n",
      "...     ...  ...  ...     ...    ...   ...  ...      ...  \n",
      "3495      0    0    0       0      0     0    0     3495  \n",
      "3496      0    0    0       0      0     0    0     3496  \n",
      "3497      0    0    0       0      0     0    0     3497  \n",
      "3498      0    0    0       0      0     0    1     3498  \n",
      "3499      0    0    0       0      0     0    0     3499  \n",
      "\n",
      "[3500 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # 1. pd.get_dummies\n",
    "    print(\"---Get Dummies--\")\n",
    "    df_tmp = pd.get_dummies(X_train['주구매상품'])\n",
    "    df_tmp['cust_id'] = np.arange(0,3500,1)\n",
    "    print(df_tmp)\n",
    "    X_train = pd.merge(X_train, df_tmp, on='cust_id')\n",
    "\n",
    "    #2. drop columns\n",
    "    X_train = X_train.drop(['환불금액', '주구매상품', '주구매지점'], axis=1)\n",
    "\n",
    "    # 2. Scaler\n",
    "    mms = MinMaxScaler()\n",
    "    X_train = pd.DataFrame(mms.fit_transform(X_train), columns=X_train.columns)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6a3f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-37eb1a049569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlg_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\bda-test\\venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5140\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5141\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'gender'"
     ]
    }
   ],
   "source": [
    "\n",
    "    y_train = y_train.gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c985aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43624480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    lg_model = LogisticRegression()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bbbc3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.svm import SVC\n",
    "\n",
    "    lg_model = SVC()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "444cc692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[ 1.00000000e+00 -8.45432740e-16  1.00000000e+00 -5.43251963e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.02506806e-15  6.44398119e-16 -7.34722753e-16  1.00000000e+00\n",
      "  1.00000000e+00 -2.17685041e-16 -6.64421753e-16  1.00000000e+00\n",
      "  1.00000000e+00  2.86228547e-16  1.00000000e+00 -6.00785461e-16\n",
      "  1.00000000e+00  3.32784647e-16 -7.27064275e-16 -6.09243661e-16\n",
      "  8.71216632e-16  1.00000000e+00 -2.48698568e-16  8.94384515e-16\n",
      "  3.75435173e-16 -5.08252534e-16 -6.90847218e-16 -7.75713486e-16\n",
      " -5.16080937e-16  1.00000000e+00 -1.36997439e-15 -2.80016342e-16\n",
      " -7.95541121e-16 -7.95193777e-16 -2.68237519e-16  1.00000000e+00\n",
      " -7.67409638e-18  1.00000000e+00  1.00000000e+00 -4.28512988e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -8.39611867e-17\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00 -4.20750419e-16  1.00000000e+00 -3.53387352e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      " -5.82288817e-16  3.66180578e-16 -9.89132320e-16  1.12281884e-16\n",
      "  1.00000000e+00  5.88524898e-16  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00 -7.31009806e-16  1.00000000e+00  8.23522404e-16\n",
      "  1.00000000e+00  4.13776608e-16  1.00000000e+00 -1.10582829e-15\n",
      "  2.72551641e-16 -3.49472191e-16  1.00000000e+00 -5.41504504e-16\n",
      " -9.29827377e-16  1.00000000e+00  1.00000000e+00 -8.07619572e-16\n",
      " -3.79499030e-16  1.00000000e+00 -6.93904673e-16  2.13512561e-17\n",
      "  4.79932612e-16  2.55808896e-18  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  6.36216887e-16 -2.29168903e-16 -7.21637516e-16\n",
      "  5.68812317e-16 -6.64127818e-16  9.40931839e-17 -5.62439867e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -1.07523546e-16\n",
      "  1.00000000e+00 -7.29888201e-16 -5.83622615e-16  1.00000000e+00\n",
      " -8.77365674e-16  1.00000000e+00 -7.31559609e-16  1.00000000e+00\n",
      "  4.50591481e-16  6.71056015e-16  1.00000000e+00 -6.73176704e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  3.60393029e-16\n",
      "  4.07490004e-16  1.00000000e+00 -3.42588557e-16  1.00000000e+00\n",
      " -2.96077961e-16  1.00000000e+00  1.00000000e+00  1.06976373e-15\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.11069671e-15\n",
      "  4.84523525e-16 -1.16419603e-15 -7.50888611e-16  2.62646439e-16\n",
      " -8.39387583e-16  5.62155296e-16  1.00000000e+00 -9.65948337e-16\n",
      " -5.07158087e-16  1.00000000e+00 -2.99176879e-16 -6.53868910e-16\n",
      "  1.00000000e+00 -4.75688615e-16  1.00000000e+00  1.00000000e+00\n",
      "  8.66917333e-17 -4.46044228e-16  1.00000000e+00  1.00000000e+00\n",
      "  8.64757803e-16 -4.46474590e-16  9.86672917e-16  3.47150168e-16\n",
      " -5.71260755e-16  4.66564531e-16 -7.84053092e-16 -5.32317941e-16\n",
      " -4.19973620e-16  4.60985178e-16 -8.61226069e-16  5.19931509e-17\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -1.06969925e-17\n",
      "  1.00000000e+00 -3.45977692e-16  1.00000000e+00  2.66992781e-16\n",
      "  1.00000000e+00  5.71592790e-16  1.00000000e+00  8.96163942e-16\n",
      " -6.63494017e-16  5.61950542e-16  3.14757519e-16  8.43496841e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      " -6.44861974e-16 -1.01730082e-15  1.00000000e+00 -8.90772605e-16\n",
      "  1.00000000e+00 -1.14036617e-15 -8.80116421e-17 -1.77757913e-16\n",
      " -1.11510155e-15  1.00000000e+00  1.00000000e+00 -2.78980592e-16\n",
      "  1.00000000e+00  3.40056527e-16  1.00000000e+00  1.00000000e+00\n",
      "  1.05793666e-16  4.61451764e-16  3.92577209e-16  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00 -1.20401736e-16  3.63603418e-16\n",
      "  3.00727106e-16  1.00000000e+00 -4.23408292e-16  5.12408793e-16\n",
      "  1.00000000e+00  3.41807157e-16  1.00000000e+00 -8.10134025e-16\n",
      "  1.00000000e+00  1.52688867e-16  1.00000000e+00  4.47026488e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -7.51943584e-17\n",
      " -6.50715401e-16  1.00000000e+00  1.00000000e+00  6.89122870e-17\n",
      " -5.33373318e-16 -1.11411988e-15  5.20069687e-16  1.00000000e+00\n",
      "  1.00000000e+00 -1.08389803e-15 -7.00680993e-16  4.01515473e-16\n",
      "  4.01866748e-16  1.00000000e+00  1.00000000e+00 -6.31097806e-16\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -5.87743947e-16\n",
      "  1.00000000e+00  2.65997658e-16  8.28546062e-16  9.57427839e-16\n",
      " -4.72335122e-16  3.82653960e-16  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00 -1.06478960e-16  1.00000000e+00 -2.06217051e-16\n",
      "  4.84565687e-16 -5.78055047e-16  3.33075790e-16  1.00000000e+00\n",
      " -2.84358904e-16  1.00000000e+00 -1.29241275e-16  1.00000000e+00\n",
      "  5.08984788e-17  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  5.52228970e-16 -7.71726794e-16  4.22688618e-16  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  4.06251302e-17\n",
      "  1.00000000e+00 -6.41389457e-16  4.70744568e-16  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.21801991e-18  4.80054255e-16\n",
      "  6.66437582e-16 -5.75135389e-16 -6.94191815e-16 -3.57025450e-17\n",
      "  1.00000000e+00 -7.96840712e-16 -9.72791362e-16  1.00000000e+00\n",
      "  1.00000000e+00 -5.70431659e-16 -2.34464188e-16  1.00000000e+00\n",
      "  1.00000000e+00 -7.61759528e-16  8.99326430e-16 -5.55040805e-16\n",
      " -6.35486909e-17 -8.24334350e-16 -1.00948228e-15  3.79531939e-16\n",
      " -5.99843343e-16  6.88058128e-16 -5.82740390e-16 -1.27929043e-15\n",
      " -7.69682230e-16  4.40956482e-17 -6.24386916e-16  1.13931464e-17\n",
      " -1.78279603e-16  1.52202641e-16 -1.18406371e-15 -2.64975002e-16\n",
      "  3.56243002e-16  4.21271408e-16  9.94344748e-17  1.00000000e+00\n",
      "  1.20435079e-15  1.00000000e+00  9.80899037e-16  5.88983598e-16\n",
      " -1.11944189e-15 -6.92288542e-16 -2.78578509e-16  1.00000000e+00\n",
      "  3.95431131e-16  1.00000000e+00 -1.37777513e-16 -4.97482038e-16\n",
      " -7.20575449e-16  1.00000000e+00  1.00000000e+00 -6.79598044e-16\n",
      "  7.21451542e-17  4.10726586e-17 -7.62670014e-16  5.64277270e-16\n",
      "  3.50242783e-16  8.19265937e-16  1.00000000e+00  5.48055418e-16\n",
      "  1.00000000e+00  1.39164989e-16 -1.78105598e-16  3.01890035e-16\n",
      "  1.00000000e+00 -5.53926770e-16  3.64666958e-16 -5.62831041e-16\n",
      "  1.00000000e+00 -8.52199558e-16  5.61764914e-16  1.00000000e+00\n",
      "  1.00000000e+00 -9.41946507e-16  6.02217896e-16 -1.03410500e-15\n",
      " -8.62540435e-16 -6.19497790e-17  3.31152083e-16 -8.65159601e-16\n",
      " -7.61518942e-16  1.00000000e+00  4.94486265e-16  1.00000000e+00\n",
      "  9.62977612e-16  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00 -7.63036034e-16\n",
      "  1.00000000e+00  3.67386392e-16  2.83058558e-16 -4.24559699e-16\n",
      "  1.11739201e-15  9.88488919e-19  3.10751188e-17  3.22626140e-16\n",
      " -6.88471995e-16  6.36401308e-16 -6.22208183e-16  1.00000000e+00\n",
      "  1.00000000e+00 -7.70345758e-17  8.37194044e-17  3.15793721e-16\n",
      "  4.37523888e-16  1.00000000e+00  1.00000000e+00 -1.06087515e-15\n",
      " -4.46470002e-16  7.91606978e-16  1.00000000e+00 -1.87203730e-17\n",
      "  4.68619878e-16  1.00000000e+00 -5.62936301e-16  5.70885150e-16\n",
      " -5.60427926e-16  3.86155659e-16  1.00000000e+00  1.00000000e+00\n",
      " -5.07787175e-16 -6.36118067e-16  3.96466237e-16  4.03904129e-16\n",
      " -8.76803245e-16 -6.49989116e-16  3.72730956e-16  1.00000000e+00\n",
      "  1.00000000e+00  1.19321454e-15 -6.93052582e-16  1.00000000e+00\n",
      "  1.00000000e+00 -4.89078222e-17 -9.23781601e-17 -2.40325495e-16\n",
      "  1.00000000e+00 -1.77073565e-16  6.74831459e-16  1.00000000e+00\n",
      " -9.95537657e-16 -8.69542262e-16 -1.04432693e-15 -1.08309734e-15\n",
      "  1.00329065e-15  4.31019209e-16  1.00000000e+00 -5.67876566e-16\n",
      "  1.00000000e+00 -8.67599553e-16  1.08426471e-15  1.00000000e+00\n",
      "  1.00000000e+00 -4.43279277e-16 -7.41478791e-16  1.00000000e+00\n",
      " -1.06123087e-15  1.00000000e+00  1.00000000e+00  3.65106452e-16]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    lg_model = LinearRegression()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fbac435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 0.9972098214285714\n",
      "평가 점수 : {} 0.9910714285714286\n",
      "cross_val_score 점수 : {} [0.99164345 0.98328691 0.98882682 0.99162011 0.9972067 ]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    lg_model = KNeighborsClassifier()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ee7b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    lg_model = DecisionTreeClassifier()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef877011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    lg_model = DecisionTreeClassifier()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "630b5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 점수 : {} 1.0\n",
      "평가 점수 : {} 1.0\n",
      "cross_val_score 점수 : {} [1. 1. 1. 1. 1.]\n",
      "[1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    lg_model = GaussianNB()\n",
    "    lg_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"훈련 점수 : {}\", lg_model.score(X_train, y_train))\n",
    "    print(\"평가 점수 : {}\", lg_model.score(X_test, y_test))\n",
    "    score = cross_val_score(lg_model,X_train, y_train, cv=5)\n",
    "    print(\"cross_val_score 점수 : {}\", score)\n",
    "    rst = lg_model.predict(X_test)\n",
    "    print(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7798fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predict = lg_model.predict(X_test)\n",
    "print(\"정확도 : {}\".format(accuracy_score(y_test, predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1110eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>총구매액</th>\n",
       "      <th>최대구매액</th>\n",
       "      <th>내점일수</th>\n",
       "      <th>내점당구매건수</th>\n",
       "      <th>주말방문비율</th>\n",
       "      <th>구매주기</th>\n",
       "      <th>환불여부</th>\n",
       "      <th>gender</th>\n",
       "      <th>가공식품</th>\n",
       "      <th>...</th>\n",
       "      <th>주방용품</th>\n",
       "      <th>차/커피</th>\n",
       "      <th>축산가공</th>\n",
       "      <th>침구/수예</th>\n",
       "      <th>캐주얼</th>\n",
       "      <th>커리어</th>\n",
       "      <th>통신/컴퓨터</th>\n",
       "      <th>트래디셔널</th>\n",
       "      <th>피혁잡화</th>\n",
       "      <th>화장품</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>0.819949</td>\n",
       "      <td>0.023244</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.063241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.150614</td>\n",
       "      <td>0.065681</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.138554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.275221</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.028458</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>0.799943</td>\n",
       "      <td>0.023316</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.068305</td>\n",
       "      <td>0.041897</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.077465</td>\n",
       "      <td>0.053617</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>0.168905</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.010241</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.023715</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.210843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.245785</td>\n",
       "      <td>0.225198</td>\n",
       "      <td>0.055906</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>0.868248</td>\n",
       "      <td>0.022576</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.022107</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.047431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.359830</td>\n",
       "      <td>0.178236</td>\n",
       "      <td>0.531690</td>\n",
       "      <td>0.122634</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cust_id      총구매액     최대구매액      내점일수   내점당구매건수    주말방문비율      구매주기  \\\n",
       "2869  0.819949  0.023244  0.006347  0.007042  0.063241  0.000000  0.349398   \n",
       "527   0.150614  0.065681  0.041983  0.049296  0.069565  0.297297  0.138554   \n",
       "963   0.275221  0.037242  0.024306  0.014085  0.028458  0.625000  0.277108   \n",
       "2799  0.799943  0.023316  0.008397  0.000000  0.000000  0.000000  0.000000   \n",
       "239   0.068305  0.041897  0.025178  0.077465  0.053617  0.306122  0.090361   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "591   0.168905  0.029703  0.010241  0.017606  0.023715  0.111111  0.210843   \n",
       "860   0.245785  0.225198  0.055906  0.176056  0.021390  0.162162  0.036145   \n",
       "3038  0.868248  0.022576  0.005921  0.000000  0.000000  0.000000  0.000000   \n",
       "2763  0.789654  0.022107  0.004242  0.010563  0.047431  0.500000  0.120482   \n",
       "15    0.004287  0.359830  0.178236  0.531690  0.122634  0.177982  0.012048   \n",
       "\n",
       "      환불여부  gender  가공식품  ...  주방용품  차/커피  축산가공  침구/수예  캐주얼  커리어  통신/컴퓨터  \\\n",
       "2869   0.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "527    1.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "963    0.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "2799   0.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "239    0.0     1.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "...    ...     ...   ...  ...   ...   ...   ...    ...  ...  ...     ...   \n",
       "591    0.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "860    1.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "3038   0.0     0.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "2763   0.0     1.0   1.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "15     1.0     1.0   0.0  ...   0.0   0.0   0.0    0.0  0.0  0.0     0.0   \n",
       "\n",
       "      트래디셔널  피혁잡화  화장품  \n",
       "2869    0.0   0.0  0.0  \n",
       "527     0.0   0.0  0.0  \n",
       "963     0.0   0.0  0.0  \n",
       "2799    0.0   0.0  0.0  \n",
       "239     0.0   0.0  0.0  \n",
       "...     ...   ...  ...  \n",
       "591     0.0   0.0  1.0  \n",
       "860     0.0   0.0  0.0  \n",
       "3038    0.0   0.0  1.0  \n",
       "2763    0.0   0.0  0.0  \n",
       "15      0.0   0.0  0.0  \n",
       "\n",
       "[448 rows x 51 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:X_test.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64e81578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a80814c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬 : [[275   0]\n",
      " [  0 173]]\n",
      "정밀도 : 1.0\n",
      "재현율 : 1.0\n",
      "F1 : 1.0\n",
      "ROC : 1.0\n"
     ]
    }
   ],
   "source": [
    "confmat = confusion_matrix(y_test, predict)\n",
    "p_score = precision_score(y_test, predict)\n",
    "r_score = recall_score(y_test, predict)\n",
    "f1 = f1_score(y_test, predict)\n",
    "roc_auc = roc_auc_score(y_test, predict)\n",
    "\n",
    "print(\"혼동 행렬 : {}\".format(confmat))\n",
    "print(\"정밀도 : {}\".format(p_score))\n",
    "print(\"재현율 : {}\".format(r_score))\n",
    "print(\"F1 : {}\".format(f1))\n",
    "print(\"ROC : {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fdb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385d8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Constructs a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It yields a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        yield different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        \n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "    \n",
      "    config_context(**new_config)\n",
      "        Context manager for global scikit-learn configuration\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=False\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "        working_memory : int, default=1024\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=True\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. Default is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default='text'\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited. This is not\n",
      "        thread-safe.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN, ...\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    0.24.1\n",
      "\n",
      "FILE\n",
      "    c:\\bda-test\\venv\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410257ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.neighbors in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.neighbors\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.neighbors` module implements the k-nearest neighbors\n",
      "    algorithm.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _ball_tree\n",
      "    _base\n",
      "    _classification\n",
      "    _dist_metrics\n",
      "    _graph\n",
      "    _kd_tree\n",
      "    _kde\n",
      "    _lof\n",
      "    _nca\n",
      "    _nearest_centroid\n",
      "    _quad_tree\n",
      "    _regression\n",
      "    _typedefs\n",
      "    _unsupervised\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.neighbors._dist_metrics.DistanceMetric\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.neighbors._kde.KernelDensity\n",
      "        sklearn.neighbors._nca.NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.neighbors._nearest_centroid.NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._nearest_centroid.NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._nca.NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.neighbors._ball_tree.BinaryTree(builtins.object)\n",
      "        sklearn.neighbors._ball_tree.BallTree\n",
      "    sklearn.neighbors._base.KNeighborsMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._base.NeighborsBase(sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.neighbors._classification.KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._lof.LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._base.RadiusNeighborsMixin(builtins.object)\n",
      "        sklearn.neighbors._classification.RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._graph.RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._regression.RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "        sklearn.neighbors._unsupervised.NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "    sklearn.neighbors._kd_tree.BinaryTree(builtins.object)\n",
      "        sklearn.neighbors._kd_tree.KDTree\n",
      "    \n",
      "    class BallTree(BinaryTree)\n",
      "     |  BallTree(X, leaf_size=40, metric='minkowski', **kwargs)\n",
      "     |  \n",
      "     |  BallTree for fast generalized N-point problems\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  X : array-like of shape (n_samples, n_features)\n",
      "     |      n_samples is the number of points in the data set, and\n",
      "     |      n_features is the dimension of the parameter space.\n",
      "     |      Note: if X is a C-contiguous array of doubles then data will\n",
      "     |      not be copied. Otherwise, an internal copy will be made.\n",
      "     |  \n",
      "     |  leaf_size : positive int, default=40\n",
      "     |      Number of points at which to switch to brute-force. Changing\n",
      "     |      leaf_size will not affect the results of a query, but can\n",
      "     |      significantly impact the speed of a query and the memory required\n",
      "     |      to store the constructed tree.  The amount of memory needed to\n",
      "     |      store the tree scales as approximately n_samples / leaf_size.\n",
      "     |      For a specified ``leaf_size``, a leaf node is guaranteed to\n",
      "     |      satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n",
      "     |      the case that ``n_samples < leaf_size``.\n",
      "     |  \n",
      "     |  metric : str or DistanceMetric object\n",
      "     |      the distance metric to use for the tree.  Default='minkowski'\n",
      "     |      with p=2 (that is, a euclidean metric). See the documentation\n",
      "     |      of the DistanceMetric class for a list of available metrics.\n",
      "     |      ball_tree.valid_metrics gives a list of the metrics which\n",
      "     |      are valid for BallTree.\n",
      "     |  \n",
      "     |  Additional keywords are passed to the distance metric class.\n",
      "     |  Note: Callable functions in the metric parameter are NOT supported for KDTree\n",
      "     |  and Ball Tree. Function call overhead will result in very poor performance.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  data : memory view\n",
      "     |      The training data\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Query for k-nearest neighbors\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)              # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Pickle and Unpickle a tree.  Note that the state of the tree is saved in the\n",
      "     |  pickle operation: the tree needs not be rebuilt upon unpickling.\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> import pickle\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)        # doctest: +SKIP\n",
      "     |      >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n",
      "     |      >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Query for neighbors within a given radius\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = BallTree(X, leaf_size=2)     # doctest: +SKIP\n",
      "     |      >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n",
      "     |      3\n",
      "     |      >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of neighbors within distance 0.3\n",
      "     |      [3 0 1]\n",
      "     |  \n",
      "     |  \n",
      "     |  Compute a gaussian kernel density estimate:\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(42)\n",
      "     |      >>> X = rng.random_sample((100, 3))\n",
      "     |      >>> tree = BallTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n",
      "     |      array([ 6.94114649,  7.83281226,  7.2071716 ])\n",
      "     |  \n",
      "     |  Compute a two-point auto-correlation function\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((30, 3))\n",
      "     |      >>> r = np.linspace(0, 1, 5)\n",
      "     |      >>> tree = BallTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.two_point_correlation(X, r)\n",
      "     |      array([ 30,  62, 278, 580, 820])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BallTree\n",
      "     |      BinaryTree\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BinaryTree:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  get_arrays(...)\n",
      "     |      get_arrays(self)\n",
      "     |      \n",
      "     |      Get data and node arrays.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arrays: tuple of array\n",
      "     |          Arrays for storing tree data, index, node data and node bounds.\n",
      "     |  \n",
      "     |  get_n_calls(...)\n",
      "     |      get_n_calls(self)\n",
      "     |      \n",
      "     |      Get number of calls.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_calls: int\n",
      "     |          number of distance computation calls\n",
      "     |  \n",
      "     |  get_tree_stats(...)\n",
      "     |      get_tree_stats(self)\n",
      "     |      \n",
      "     |      Get tree status.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tree_stats: tuple of int\n",
      "     |          (number of trims, number of leaves, number of splits)\n",
      "     |  \n",
      "     |  kernel_density(...)\n",
      "     |      kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1E-8,\n",
      "     |                     breadth_first=True, return_log=False)\n",
      "     |      \n",
      "     |      Compute the kernel density estimate at points X with the given kernel,\n",
      "     |      using the distance metric specified at tree creation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      h : float\n",
      "     |          the bandwidth of the kernel\n",
      "     |      kernel : str, default=\"gaussian\"\n",
      "     |          specify the kernel to use.  Options are\n",
      "     |          - 'gaussian'\n",
      "     |          - 'tophat'\n",
      "     |          - 'epanechnikov'\n",
      "     |          - 'exponential'\n",
      "     |          - 'linear'\n",
      "     |          - 'cosine'\n",
      "     |          Default is kernel = 'gaussian'\n",
      "     |      atol, rtol : float, default=0, 1e-8\n",
      "     |          Specify the desired relative and absolute tolerance of the result.\n",
      "     |          If the true result is K_true, then the returned result K_ret\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is zero (i.e. machine precision) for both.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          If True, use a breadth-first search.  If False (default) use a\n",
      "     |          depth-first search.  Breadth-first is generally faster for\n",
      "     |          compact kernels and/or high tolerances.\n",
      "     |      return_log : bool, default=False\n",
      "     |          Return the logarithm of the result.  This can be more accurate\n",
      "     |          than returning the result itself for narrow kernels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape X.shape[:-1]\n",
      "     |          The array of (log)-density evaluations\n",
      "     |  \n",
      "     |  query(...)\n",
      "     |      query(X, k=1, return_distance=True,\n",
      "     |            dualtree=False, breadth_first=False)\n",
      "     |      \n",
      "     |      query the tree for the k nearest neighbors\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      k : int, default=1\n",
      "     |          The number of nearest neighbors to return\n",
      "     |      return_distance : bool, default=True\n",
      "     |          if True, return a tuple (d, i) of distances and indices\n",
      "     |          if False, return array i\n",
      "     |      dualtree : bool, default=False\n",
      "     |          if True, use the dual tree formalism for the query: a tree is\n",
      "     |          built for the query points, and the pair of trees is used to\n",
      "     |          efficiently search this space.  This can lead to better\n",
      "     |          performance as the number of points grows large.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          if True, then query the nodes in a breadth-first manner.\n",
      "     |          Otherwise, query the nodes in a depth-first manner.\n",
      "     |      sort_results : bool, default=True\n",
      "     |          if True, then distances and indices of each point are sorted\n",
      "     |          on return, so that the first column contains the closest points.\n",
      "     |          Otherwise, neighbors are returned in an arbitrary order.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      i    : if return_distance == False\n",
      "     |      (d,i) : if return_distance == True\n",
      "     |      \n",
      "     |      d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n",
      "     |          Each entry gives the list of distances to the neighbors of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n",
      "     |          Each entry gives the list of indices of neighbors of the\n",
      "     |          corresponding point.\n",
      "     |  \n",
      "     |  query_radius(...)\n",
      "     |      query_radius(X, r, return_distance=False,\n",
      "     |      count_only=False, sort_results=False)\n",
      "     |      \n",
      "     |      query the tree for neighbors within a radius r\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      r : distance within which neighbors are returned\n",
      "     |          r can be a single value, or an array of values of shape\n",
      "     |          x.shape[:-1] if different radii are desired for each point.\n",
      "     |      return_distance : bool, default=False\n",
      "     |          if True,  return distances to neighbors of each point\n",
      "     |          if False, return only neighbors\n",
      "     |          Note that unlike the query() method, setting return_distance=True\n",
      "     |          here adds to the computation time.  Not all distances need to be\n",
      "     |          calculated explicitly for return_distance=False.  Results are\n",
      "     |          not sorted by default: see ``sort_results`` keyword.\n",
      "     |      count_only : bool, default=False\n",
      "     |          if True,  return only the count of points within distance r\n",
      "     |          if False, return the indices of all points within distance r\n",
      "     |          If return_distance==True, setting count_only=True will\n",
      "     |          result in an error.\n",
      "     |      sort_results : bool, default=False\n",
      "     |          if True, the distances and indices will be sorted before being\n",
      "     |          returned.  If False, the results will not be sorted.  If\n",
      "     |          return_distance == False, setting sort_results = True will\n",
      "     |          result in an error.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      count       : if count_only == True\n",
      "     |      ind         : if count_only == False and return_distance == False\n",
      "     |      (ind, dist) : if count_only == False and return_distance == True\n",
      "     |      \n",
      "     |      count : ndarray of shape X.shape[:-1], dtype=int\n",
      "     |          Each entry gives the number of neighbors within a distance r of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      ind : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy integer array listing the indices of\n",
      "     |          neighbors of the corresponding point.  Note that unlike\n",
      "     |          the results of a k-neighbors query, the returned neighbors\n",
      "     |          are not sorted by distance by default.\n",
      "     |      \n",
      "     |      dist : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy double array listing the distances\n",
      "     |          corresponding to indices in i.\n",
      "     |  \n",
      "     |  reset_n_calls(...)\n",
      "     |      reset_n_calls(self)\n",
      "     |      \n",
      "     |      Reset number of calls to 0.\n",
      "     |  \n",
      "     |  two_point_correlation(...)\n",
      "     |      two_point_correlation(X, r, dualtree=False)\n",
      "     |      \n",
      "     |      Compute the two-point correlation function\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      r : array-like\n",
      "     |          A one-dimensional array of distances\n",
      "     |      dualtree : bool, default=False\n",
      "     |          If True, use a dualtree algorithm.  Otherwise, use a single-tree\n",
      "     |          algorithm.  Dual tree algorithms can have better scaling for\n",
      "     |          large N.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      counts : ndarray\n",
      "     |          counts[i] contains the number of pairs of points with distance\n",
      "     |          less than or equal to r[i]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BinaryTree:\n",
      "     |  \n",
      "     |  data\n",
      "     |  \n",
      "     |  idx_array\n",
      "     |  \n",
      "     |  node_bounds\n",
      "     |  \n",
      "     |  node_data\n",
      "     |  \n",
      "     |  sample_weight\n",
      "     |  \n",
      "     |  sum_weight\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BinaryTree:\n",
      "     |  \n",
      "     |  valid_metrics = ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'ci...\n",
      "    \n",
      "    class DistanceMetric(builtins.object)\n",
      "     |  DistanceMetric class\n",
      "     |  \n",
      "     |  This class provides a uniform interface to fast distance metric\n",
      "     |  functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
      "     |  class method and the metric string identifier (see below).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.neighbors import DistanceMetric\n",
      "     |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      "     |  >>> X = [[0, 1, 2],\n",
      "     |           [3, 4, 5]]\n",
      "     |  >>> dist.pairwise(X)\n",
      "     |  array([[ 0.        ,  5.19615242],\n",
      "     |         [ 5.19615242,  0.        ]])\n",
      "     |  \n",
      "     |  Available Metrics\n",
      "     |  \n",
      "     |  The following lists the string metric identifiers and the associated\n",
      "     |  distance metric classes:\n",
      "     |  \n",
      "     |  **Metrics intended for real-valued vector spaces:**\n",
      "     |  \n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  identifier      class name            args      distance function\n",
      "     |  --------------  --------------------  --------  -------------------------------\n",
      "     |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      "     |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      "     |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      "     |  \"minkowski\"     MinkowskiDistance     p         ``sum(|x - y|^p)^(1/p)``\n",
      "     |  \"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
      "     |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      "     |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  \n",
      "     |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      "     |  distance metric requires data in the form of [latitude, longitude] and both\n",
      "     |  inputs and outputs are in units of radians.\n",
      "     |  \n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  identifier    class name          distance function\n",
      "     |  ------------  ------------------  ---------------------------------------------------------------\n",
      "     |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  \n",
      "     |  \n",
      "     |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      "     |  for integer-valued vectors, these are also valid metrics in the case of\n",
      "     |  real-valued vectors.\n",
      "     |  \n",
      "     |  =============  ====================  ========================================\n",
      "     |  identifier     class name            distance function\n",
      "     |  -------------  --------------------  ----------------------------------------\n",
      "     |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      "     |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      "     |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      "     |  =============  ====================  ========================================\n",
      "     |  \n",
      "     |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      "     |  is evaluated to \"True\".  In the listings below, the following\n",
      "     |  abbreviations are used:\n",
      "     |  \n",
      "     |   - N  : number of dimensions\n",
      "     |   - NTT : number of dims in which both values are True\n",
      "     |   - NTF : number of dims in which the first value is True, second is False\n",
      "     |   - NFT : number of dims in which the first value is False, second is True\n",
      "     |   - NFF : number of dims in which both values are False\n",
      "     |   - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
      "     |   - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      "     |  \n",
      "     |  =================  =======================  ===============================\n",
      "     |  identifier         class name               distance function\n",
      "     |  -----------------  -----------------------  -------------------------------\n",
      "     |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      "     |  \"matching\"         MatchingDistance         NNEQ / N\n",
      "     |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      "     |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      "     |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      "     |  \"russellrao\"       RussellRaoDistance       NNZ / N\n",
      "     |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      "     |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      "     |  =================  =======================  ===============================\n",
      "     |  \n",
      "     |  **User-defined distance:**\n",
      "     |  \n",
      "     |  ===========    ===============    =======\n",
      "     |  identifier     class name         args\n",
      "     |  -----------    ---------------    -------\n",
      "     |  \"pyfunc\"       PyFuncDistance     func\n",
      "     |  ===========    ===============    =======\n",
      "     |  \n",
      "     |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      "     |  arrays, and returns a distance.  Note that in order to be used within\n",
      "     |  the BallTree, the distance must be a true metric:\n",
      "     |  i.e. it must satisfy the following properties\n",
      "     |  \n",
      "     |  1) Non-negativity: d(x, y) >= 0\n",
      "     |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      "     |  3) Symmetry: d(x, y) = d(y, x)\n",
      "     |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      "     |  \n",
      "     |  Because of the Python object overhead involved in calling the python\n",
      "     |  function, this will be fairly slow, but it will have the same\n",
      "     |  scaling as other distances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  dist_to_rdist(...)\n",
      "     |      Convert the true distance to the reduced distance.\n",
      "     |      \n",
      "     |      The reduced distance, defined for some metrics, is a computationally\n",
      "     |      more efficient measure which preserves the rank of the true distance.\n",
      "     |      For example, in the Euclidean distance metric, the reduced distance\n",
      "     |      is the squared-euclidean distance.\n",
      "     |  \n",
      "     |  get_metric(...) from builtins.type\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : string or class name\n",
      "     |          The distance metric to use\n",
      "     |      **kwargs\n",
      "     |          additional arguments will be passed to the requested metric\n",
      "     |  \n",
      "     |  pairwise(...)\n",
      "     |      Compute the pairwise distances between X and Y\n",
      "     |      \n",
      "     |      This is a convenience routine for the sake of testing.  For many\n",
      "     |      metrics, the utilities in scipy.spatial.distance.cdist and\n",
      "     |      scipy.spatial.distance.pdist will be faster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like\n",
      "     |          Array of shape (Nx, D), representing Nx points in D dimensions.\n",
      "     |      Y : array-like (optional)\n",
      "     |          Array of shape (Ny, D), representing Ny points in D dimensions.\n",
      "     |          If not specified, then Y=X.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dist : ndarray\n",
      "     |          The shape (Nx, Ny) array of pairwise distances between points in\n",
      "     |          X and Y.\n",
      "     |  \n",
      "     |  rdist_to_dist(...)\n",
      "     |      Convert the Reduced distance to the true distance.\n",
      "     |      \n",
      "     |      The reduced distance, defined for some metrics, is a computationally\n",
      "     |      more efficient measure which preserves the rank of the true distance.\n",
      "     |      For example, in the Euclidean distance metric, the reduced distance\n",
      "     |      is the squared-euclidean distance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "    \n",
      "    class KDTree(BinaryTree)\n",
      "     |  KDTree(X, leaf_size=40, metric='minkowski', **kwargs)\n",
      "     |  \n",
      "     |  KDTree for fast generalized N-point problems\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  X : array-like of shape (n_samples, n_features)\n",
      "     |      n_samples is the number of points in the data set, and\n",
      "     |      n_features is the dimension of the parameter space.\n",
      "     |      Note: if X is a C-contiguous array of doubles then data will\n",
      "     |      not be copied. Otherwise, an internal copy will be made.\n",
      "     |  \n",
      "     |  leaf_size : positive int, default=40\n",
      "     |      Number of points at which to switch to brute-force. Changing\n",
      "     |      leaf_size will not affect the results of a query, but can\n",
      "     |      significantly impact the speed of a query and the memory required\n",
      "     |      to store the constructed tree.  The amount of memory needed to\n",
      "     |      store the tree scales as approximately n_samples / leaf_size.\n",
      "     |      For a specified ``leaf_size``, a leaf node is guaranteed to\n",
      "     |      satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n",
      "     |      the case that ``n_samples < leaf_size``.\n",
      "     |  \n",
      "     |  metric : str or DistanceMetric object\n",
      "     |      the distance metric to use for the tree.  Default='minkowski'\n",
      "     |      with p=2 (that is, a euclidean metric). See the documentation\n",
      "     |      of the DistanceMetric class for a list of available metrics.\n",
      "     |      kd_tree.valid_metrics gives a list of the metrics which\n",
      "     |      are valid for KDTree.\n",
      "     |  \n",
      "     |  Additional keywords are passed to the distance metric class.\n",
      "     |  Note: Callable functions in the metric parameter are NOT supported for KDTree\n",
      "     |  and Ball Tree. Function call overhead will result in very poor performance.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  data : memory view\n",
      "     |      The training data\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Query for k-nearest neighbors\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Pickle and Unpickle a tree.  Note that the state of the tree is saved in the\n",
      "     |  pickle operation: the tree needs not be rebuilt upon unpickling.\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> import pickle\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n",
      "     |      >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n",
      "     |      >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n",
      "     |      >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of 3 closest neighbors\n",
      "     |      [0 3 1]\n",
      "     |      >>> print(dist)  # distances to 3 closest neighbors\n",
      "     |      [ 0.          0.19662693  0.29473397]\n",
      "     |  \n",
      "     |  Query for neighbors within a given radius\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n",
      "     |      >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n",
      "     |      >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n",
      "     |      3\n",
      "     |      >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n",
      "     |      >>> print(ind)  # indices of neighbors within distance 0.3\n",
      "     |      [3 0 1]\n",
      "     |  \n",
      "     |  \n",
      "     |  Compute a gaussian kernel density estimate:\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(42)\n",
      "     |      >>> X = rng.random_sample((100, 3))\n",
      "     |      >>> tree = KDTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n",
      "     |      array([ 6.94114649,  7.83281226,  7.2071716 ])\n",
      "     |  \n",
      "     |  Compute a two-point auto-correlation function\n",
      "     |  \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> rng = np.random.RandomState(0)\n",
      "     |      >>> X = rng.random_sample((30, 3))\n",
      "     |      >>> r = np.linspace(0, 1, 5)\n",
      "     |      >>> tree = KDTree(X)                # doctest: +SKIP\n",
      "     |      >>> tree.two_point_correlation(X, r)\n",
      "     |      array([ 30,  62, 278, 580, 820])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KDTree\n",
      "     |      BinaryTree\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BinaryTree:\n",
      "     |  \n",
      "     |  __getstate__(...)\n",
      "     |      get state for pickling\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      reduce method used for pickling\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |      set state for pickling\n",
      "     |  \n",
      "     |  get_arrays(...)\n",
      "     |      get_arrays(self)\n",
      "     |      \n",
      "     |      Get data and node arrays.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arrays: tuple of array\n",
      "     |          Arrays for storing tree data, index, node data and node bounds.\n",
      "     |  \n",
      "     |  get_n_calls(...)\n",
      "     |      get_n_calls(self)\n",
      "     |      \n",
      "     |      Get number of calls.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_calls: int\n",
      "     |          number of distance computation calls\n",
      "     |  \n",
      "     |  get_tree_stats(...)\n",
      "     |      get_tree_stats(self)\n",
      "     |      \n",
      "     |      Get tree status.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tree_stats: tuple of int\n",
      "     |          (number of trims, number of leaves, number of splits)\n",
      "     |  \n",
      "     |  kernel_density(...)\n",
      "     |      kernel_density(self, X, h, kernel='gaussian', atol=0, rtol=1E-8,\n",
      "     |                     breadth_first=True, return_log=False)\n",
      "     |      \n",
      "     |      Compute the kernel density estimate at points X with the given kernel,\n",
      "     |      using the distance metric specified at tree creation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      h : float\n",
      "     |          the bandwidth of the kernel\n",
      "     |      kernel : str, default=\"gaussian\"\n",
      "     |          specify the kernel to use.  Options are\n",
      "     |          - 'gaussian'\n",
      "     |          - 'tophat'\n",
      "     |          - 'epanechnikov'\n",
      "     |          - 'exponential'\n",
      "     |          - 'linear'\n",
      "     |          - 'cosine'\n",
      "     |          Default is kernel = 'gaussian'\n",
      "     |      atol, rtol : float, default=0, 1e-8\n",
      "     |          Specify the desired relative and absolute tolerance of the result.\n",
      "     |          If the true result is K_true, then the returned result K_ret\n",
      "     |          satisfies ``abs(K_true - K_ret) < atol + rtol * K_ret``\n",
      "     |          The default is zero (i.e. machine precision) for both.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          If True, use a breadth-first search.  If False (default) use a\n",
      "     |          depth-first search.  Breadth-first is generally faster for\n",
      "     |          compact kernels and/or high tolerances.\n",
      "     |      return_log : bool, default=False\n",
      "     |          Return the logarithm of the result.  This can be more accurate\n",
      "     |          than returning the result itself for narrow kernels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape X.shape[:-1]\n",
      "     |          The array of (log)-density evaluations\n",
      "     |  \n",
      "     |  query(...)\n",
      "     |      query(X, k=1, return_distance=True,\n",
      "     |            dualtree=False, breadth_first=False)\n",
      "     |      \n",
      "     |      query the tree for the k nearest neighbors\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      k : int, default=1\n",
      "     |          The number of nearest neighbors to return\n",
      "     |      return_distance : bool, default=True\n",
      "     |          if True, return a tuple (d, i) of distances and indices\n",
      "     |          if False, return array i\n",
      "     |      dualtree : bool, default=False\n",
      "     |          if True, use the dual tree formalism for the query: a tree is\n",
      "     |          built for the query points, and the pair of trees is used to\n",
      "     |          efficiently search this space.  This can lead to better\n",
      "     |          performance as the number of points grows large.\n",
      "     |      breadth_first : bool, default=False\n",
      "     |          if True, then query the nodes in a breadth-first manner.\n",
      "     |          Otherwise, query the nodes in a depth-first manner.\n",
      "     |      sort_results : bool, default=True\n",
      "     |          if True, then distances and indices of each point are sorted\n",
      "     |          on return, so that the first column contains the closest points.\n",
      "     |          Otherwise, neighbors are returned in an arbitrary order.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      i    : if return_distance == False\n",
      "     |      (d,i) : if return_distance == True\n",
      "     |      \n",
      "     |      d : ndarray of shape X.shape[:-1] + (k,), dtype=double\n",
      "     |          Each entry gives the list of distances to the neighbors of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      i : ndarray of shape X.shape[:-1] + (k,), dtype=int\n",
      "     |          Each entry gives the list of indices of neighbors of the\n",
      "     |          corresponding point.\n",
      "     |  \n",
      "     |  query_radius(...)\n",
      "     |      query_radius(X, r, return_distance=False,\n",
      "     |      count_only=False, sort_results=False)\n",
      "     |      \n",
      "     |      query the tree for neighbors within a radius r\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query\n",
      "     |      r : distance within which neighbors are returned\n",
      "     |          r can be a single value, or an array of values of shape\n",
      "     |          x.shape[:-1] if different radii are desired for each point.\n",
      "     |      return_distance : bool, default=False\n",
      "     |          if True,  return distances to neighbors of each point\n",
      "     |          if False, return only neighbors\n",
      "     |          Note that unlike the query() method, setting return_distance=True\n",
      "     |          here adds to the computation time.  Not all distances need to be\n",
      "     |          calculated explicitly for return_distance=False.  Results are\n",
      "     |          not sorted by default: see ``sort_results`` keyword.\n",
      "     |      count_only : bool, default=False\n",
      "     |          if True,  return only the count of points within distance r\n",
      "     |          if False, return the indices of all points within distance r\n",
      "     |          If return_distance==True, setting count_only=True will\n",
      "     |          result in an error.\n",
      "     |      sort_results : bool, default=False\n",
      "     |          if True, the distances and indices will be sorted before being\n",
      "     |          returned.  If False, the results will not be sorted.  If\n",
      "     |          return_distance == False, setting sort_results = True will\n",
      "     |          result in an error.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      count       : if count_only == True\n",
      "     |      ind         : if count_only == False and return_distance == False\n",
      "     |      (ind, dist) : if count_only == False and return_distance == True\n",
      "     |      \n",
      "     |      count : ndarray of shape X.shape[:-1], dtype=int\n",
      "     |          Each entry gives the number of neighbors within a distance r of the\n",
      "     |          corresponding point.\n",
      "     |      \n",
      "     |      ind : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy integer array listing the indices of\n",
      "     |          neighbors of the corresponding point.  Note that unlike\n",
      "     |          the results of a k-neighbors query, the returned neighbors\n",
      "     |          are not sorted by distance by default.\n",
      "     |      \n",
      "     |      dist : ndarray of shape X.shape[:-1], dtype=object\n",
      "     |          Each element is a numpy double array listing the distances\n",
      "     |          corresponding to indices in i.\n",
      "     |  \n",
      "     |  reset_n_calls(...)\n",
      "     |      reset_n_calls(self)\n",
      "     |      \n",
      "     |      Reset number of calls to 0.\n",
      "     |  \n",
      "     |  two_point_correlation(...)\n",
      "     |      two_point_correlation(X, r, dualtree=False)\n",
      "     |      \n",
      "     |      Compute the two-point correlation function\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data.\n",
      "     |      r : array-like\n",
      "     |          A one-dimensional array of distances\n",
      "     |      dualtree : bool, default=False\n",
      "     |          If True, use a dualtree algorithm.  Otherwise, use a single-tree\n",
      "     |          algorithm.  Dual tree algorithms can have better scaling for\n",
      "     |          large N.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      counts : ndarray\n",
      "     |          counts[i] contains the number of pairs of points with distance\n",
      "     |          less than or equal to r[i]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BinaryTree:\n",
      "     |  \n",
      "     |  data\n",
      "     |  \n",
      "     |  idx_array\n",
      "     |  \n",
      "     |  node_bounds\n",
      "     |  \n",
      "     |  node_data\n",
      "     |  \n",
      "     |  sample_weight\n",
      "     |  \n",
      "     |  sum_weight\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BinaryTree:\n",
      "     |  \n",
      "     |  valid_metrics = ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'ci...\n",
      "    \n",
      "    class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Classifier implementing the k-nearest neighbors vote.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      the distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |      Doesn't affect :meth:`fit` method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      Class labels known to the classifier\n",
      "     |  \n",
      "     |  effective_metric_ : str or callble\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  outputs_2d_ : bool\n",
      "     |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      "     |      otherwise True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      "     |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(neigh.predict([[1.1]]))\n",
      "     |  [0]\n",
      "     |  >>> print(neigh.predict_proba([[0.9]]))\n",
      "     |  [[0.66666667 0.33333333]]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RadiusNeighborsClassifier\n",
      "     |  KNeighborsRegressor\n",
      "     |  RadiusNeighborsRegressor\n",
      "     |  NearestNeighbors\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "     |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      "     |     but different labels, the results will depend on the ordering of the\n",
      "     |     training data.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsClassifier\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the k-nearest neighbors classifier from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsClassifier\n",
      "     |          The fitted k-nearest neighbors classifier.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the class labels for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      "     |          Class labels for each data sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test data X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n",
      "     |          of such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. Classes are ordered\n",
      "     |          by lexicographic order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Finds the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Regression based on k-nearest neighbors.\n",
      "     |  \n",
      "     |  The target is predicted by local interpolation of the targets\n",
      "     |  associated of the nearest neighbors in the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      the distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |      Doesn't affect :meth:`fit` method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric to use. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      "     |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  KNeighborsRegressor(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0.5]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NearestNeighbors\n",
      "     |  RadiusNeighborsRegressor\n",
      "     |  KNeighborsClassifier\n",
      "     |  RadiusNeighborsClassifier\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      "     |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      "     |     different labels, the results will depend on the ordering of the\n",
      "     |     training data.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsRegressor\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the k-nearest neighbors regressor from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsRegressor\n",
      "     |          The fitted k-nearest neighbors regressor.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target for the provided data\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Finds the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KNeighborsTransformer(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Transform X into a (weighted) graph of k nearest neighbors\n",
      "     |  \n",
      "     |  The transformed data is a sparse graph as returned by kneighbors_graph.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <neighbors_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  mode : {'distance', 'connectivity'}, default='distance'\n",
      "     |      Type of returned matrix: 'connectivity' will return the connectivity\n",
      "     |      matrix with ones and zeros, and 'distance' will return the distances\n",
      "     |      between neighbors according to the given metric.\n",
      "     |  \n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors for each sample in the transformed sparse graph.\n",
      "     |      For compatibility reasons, as each sample is considered as its own\n",
      "     |      neighbor, one extra neighbor will be computed when mode == 'distance'.\n",
      "     |      In this case, the sparse graph contains (n_neighbors + 1) neighbors.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Distance matrices are not supported.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=1\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.manifold import Isomap\n",
      "     |  >>> from sklearn.neighbors import KNeighborsTransformer\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> estimator = make_pipeline(\n",
      "     |  ...     KNeighborsTransformer(n_neighbors=5, mode='distance'),\n",
      "     |  ...     Isomap(neighbors_algorithm='precomputed'))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNeighborsTransformer\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, mode='distance', n_neighbors=5, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the k-nearest neighbors transformer from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : KNeighborsTransformer\n",
      "     |          The fitted k-nearest neighbors transformer.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : ignored\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples, n_samples)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples_transform, n_features)\n",
      "     |          Sample data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Finds the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class KernelDensity(sklearn.base.BaseEstimator)\n",
      "     |  Kernel Density Estimation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <kernel_density>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  bandwidth : float, default=1.0\n",
      "     |      The bandwidth of the kernel.\n",
      "     |  \n",
      "     |  algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto'\n",
      "     |      The tree algorithm to use.\n",
      "     |  \n",
      "     |  kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian'\n",
      "     |      The kernel to use.\n",
      "     |  \n",
      "     |  metric : str, default='euclidian'\n",
      "     |      The distance metric to use.  Note that not all metrics are\n",
      "     |      valid with all algorithms.  Refer to the documentation of\n",
      "     |      :class:`BallTree` and :class:`KDTree` for a description of\n",
      "     |      available algorithms.  Note that the normalization of the density\n",
      "     |      output is correct only for the Euclidean distance metric. Default\n",
      "     |      is 'euclidean'.\n",
      "     |  \n",
      "     |  atol : float, default=0\n",
      "     |      The desired absolute tolerance of the result.  A larger tolerance will\n",
      "     |      generally lead to faster execution.\n",
      "     |  \n",
      "     |  rtol : float, default=0\n",
      "     |      The desired relative tolerance of the result.  A larger tolerance will\n",
      "     |      generally lead to faster execution.\n",
      "     |  \n",
      "     |  breadth_first : bool, default=True\n",
      "     |      If true (default), use a breadth-first approach to the problem.\n",
      "     |      Otherwise use a depth-first approach.\n",
      "     |  \n",
      "     |  leaf_size : int, default=40\n",
      "     |      Specify the leaf size of the underlying tree.  See :class:`BallTree`\n",
      "     |      or :class:`KDTree` for details.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional parameters to be passed to the tree for use with the\n",
      "     |      metric.  For more information, see the documentation of\n",
      "     |      :class:`BallTree` or :class:`KDTree`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  tree_ : ``BinaryTree`` instance\n",
      "     |      The tree algorithm for fast generalized N-point problems.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point\n",
      "     |      problems.\n",
      "     |  sklearn.neighbors.BallTree : Ball tree for fast generalized N-point\n",
      "     |      problems.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Compute a gaussian kernel density estimate with a fixed bandwidth.\n",
      "     |  \n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(42)\n",
      "     |  >>> X = rng.random_sample((100, 3))\n",
      "     |  >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n",
      "     |  >>> log_density = kde.score_samples(X[:3])\n",
      "     |  >>> log_density\n",
      "     |  array([-1.52955942, -1.51462041, -1.60244657])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KernelDensity\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, bandwidth=1.0, algorithm='auto', kernel='gaussian', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit the Kernel Density model on the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of n_features-dimensional data points.  Each row\n",
      "     |          corresponds to a single data point.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          List of sample weights attached to the data X.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns instance of object.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Currently, this is implemented only for gaussian and tophat kernels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int, default=1\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : int, RandomState instance or None, default=None\n",
      "     |          Determines random number generation used to generate\n",
      "     |          random samples. Pass an int for reproducible results\n",
      "     |          across multiple function calls.\n",
      "     |          See :term: `Glossary <random_state>`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of samples.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Compute the total log probability density under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          List of n_features-dimensional data points.  Each row\n",
      "     |          corresponds to a single data point.\n",
      "     |      \n",
      "     |      y : None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          :class:`~sklearn.pipeline.Pipeline`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Total log-likelihood of the data in X. This is normalized to be a\n",
      "     |          probability density, so the value will be low for high-dimensional\n",
      "     |          data.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Evaluate the log density model on the data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          An array of points to query.  Last dimension should match dimension\n",
      "     |          of training data (n_features).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      density : ndarray of shape (n_samples,)\n",
      "     |          The array of log(density) evaluations. These are normalized to be\n",
      "     |          probability densities, so values will be low for high-dimensional\n",
      "     |          data.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Unsupervised Outlier Detection using Local Outlier Factor (LOF)\n",
      "     |  \n",
      "     |  The anomaly score of each sample is called Local Outlier Factor.\n",
      "     |  It measures the local deviation of density of a given sample with\n",
      "     |  respect to its neighbors.\n",
      "     |  It is local in that the anomaly score depends on how isolated the object\n",
      "     |  is with respect to the surrounding neighborhood.\n",
      "     |  More precisely, locality is given by k-nearest neighbors, whose distance\n",
      "     |  is used to estimate the local density.\n",
      "     |  By comparing the local density of a sample to the local densities of\n",
      "     |  its neighbors, one can identify samples that have a substantially lower\n",
      "     |  density than their neighbors. These are considered outliers.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=20\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |      If n_neighbors is larger than the number of samples provided,\n",
      "     |      all samples will be used.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      "     |      affect the speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree. The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      metric used for the distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square. X may be a sparse matrix, in which case only \"nonzero\"\n",
      "     |      elements may be considered neighbors.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics:\n",
      "     |      https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this\n",
      "     |      is equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  contamination : 'auto' or float, default='auto'\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. When fitting this is used to define the\n",
      "     |      threshold on the scores of the samples.\n",
      "     |  \n",
      "     |      - if 'auto', the threshold is determined as in the\n",
      "     |        original paper,\n",
      "     |      - if a float, the contamination should be in the range [0, 0.5].\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``contamination`` changed from 0.1\n",
      "     |         to ``'auto'``.\n",
      "     |  \n",
      "     |  novelty : bool, default=False\n",
      "     |      By default, LocalOutlierFactor is only meant to be used for outlier\n",
      "     |      detection (novelty=False). Set novelty to True if you want to use\n",
      "     |      LocalOutlierFactor for novelty detection. In this case be aware that\n",
      "     |      that you should only use predict, decision_function and score_samples\n",
      "     |      on new unseen data and not on the training set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  negative_outlier_factor_ : ndarray of shape (n_samples,)\n",
      "     |      The opposite LOF of the training samples. The higher, the more normal.\n",
      "     |      Inliers tend to have a LOF score close to 1\n",
      "     |      (``negative_outlier_factor_`` close to -1), while outliers tend to have\n",
      "     |      a larger LOF score.\n",
      "     |  \n",
      "     |      The local outlier factor (LOF) of a sample captures its\n",
      "     |      supposed 'degree of abnormality'.\n",
      "     |      It is the average of the ratio of the local reachability density of\n",
      "     |      a sample and those of its k-nearest neighbors.\n",
      "     |  \n",
      "     |  n_neighbors_ : int\n",
      "     |      The actual number of neighbors used for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to obtain binary labels from the raw scores.\n",
      "     |      Observations having a negative_outlier_factor smaller than `offset_`\n",
      "     |      are detected as abnormal.\n",
      "     |      The offset is set to -1.5 (inliers score around -1), except when a\n",
      "     |      contamination parameter different than \"auto\" is provided. In that\n",
      "     |      case, the offset is defined in such a way we obtain the expected\n",
      "     |      number of outliers in training.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  effective_metric_ : str\n",
      "     |      The effective metric used for the distance computation.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      The effective additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      It is the number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.neighbors import LocalOutlierFactor\n",
      "     |  >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n",
      "     |  >>> clf = LocalOutlierFactor(n_neighbors=2)\n",
      "     |  >>> clf.fit_predict(X)\n",
      "     |  array([ 1,  1, -1,  1])\n",
      "     |  >>> clf.negative_outlier_factor_\n",
      "     |  array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n",
      "     |         LOF: identifying density-based local outliers. In ACM sigmod record.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LocalOutlierFactor\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the local outlier factor detector from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : LocalOutlierFactor\n",
      "     |          The fitted local outlier factor detector.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  decision_function\n",
      "     |      Shifted opposite of the Local Outlier Factor of X.\n",
      "     |      \n",
      "     |      Bigger is better, i.e. large values correspond to inliers.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      The shift offset allows a zero threshold for being an outlier.\n",
      "     |      The argument X is supposed to contain *new data*: if X contains a\n",
      "     |      point from training, it considers the later in its own neighborhood.\n",
      "     |      Also, the samples in X are not considered in the neighborhood of any\n",
      "     |      point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      shifted_opposite_lof_scores : ndarray of shape (n_samples,)\n",
      "     |          The shifted opposite of the Local Outlier Factor of each input\n",
      "     |          samples. The lower, the more abnormal. Negative scores represent\n",
      "     |          outliers, positive scores represent inliers.\n",
      "     |  \n",
      "     |  fit_predict\n",
      "     |      Fits the model to the training set X and returns the labels.\n",
      "     |      \n",
      "     |      **Not available for novelty detection (when novelty is set to True).**\n",
      "     |      Label is 1 for an inlier and -1 for an outlier according to the LOF\n",
      "     |      score and the contamination parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. to the training samples.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          Returns -1 for anomalies/outliers and 1 for inliers.\n",
      "     |  \n",
      "     |  predict\n",
      "     |      Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      This method allows to generalize prediction to *new observations* (not\n",
      "     |      in the training set).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. to the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          Returns -1 for anomalies/outliers and +1 for inliers.\n",
      "     |  \n",
      "     |  score_samples\n",
      "     |      Opposite of the Local Outlier Factor of X.\n",
      "     |      \n",
      "     |      It is the opposite as bigger is better, i.e. large values correspond\n",
      "     |      to inliers.\n",
      "     |      \n",
      "     |      **Only available for novelty detection (when novelty is set to True).**\n",
      "     |      The argument X is supposed to contain *new data*: if X contains a\n",
      "     |      point from training, it considers the later in its own neighborhood.\n",
      "     |      Also, the samples in X are not considered in the neighborhood of any\n",
      "     |      point.\n",
      "     |      The score_samples on training data is available by considering the\n",
      "     |      the ``negative_outlier_factor_`` attribute.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The query sample or samples to compute the Local Outlier Factor\n",
      "     |          w.r.t. the training samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      opposite_lof_scores : ndarray of shape (n_samples,)\n",
      "     |          The opposite of the Local Outlier Factor of each input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Finds the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NearestCentroid(sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      "     |  Nearest centroid classifier.\n",
      "     |  \n",
      "     |  Each class is represented by its centroid, with test samples classified to\n",
      "     |  the class with the nearest centroid.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  metric : str or callable\n",
      "     |      The metric to use when calculating distance between instances in a\n",
      "     |      feature array. If metric is a string or callable, it must be one of\n",
      "     |      the options allowed by metrics.pairwise.pairwise_distances for its\n",
      "     |      metric parameter.\n",
      "     |      The centroids for the samples corresponding to each class is the point\n",
      "     |      from which the sum of the distances (according to the metric) of all\n",
      "     |      samples that belong to that particular class are minimized.\n",
      "     |      If the \"manhattan\" metric is provided, this centroid is the median and\n",
      "     |      for all other metrics, the centroid is now set to be the mean.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          ``metric='precomputed'`` was deprecated and now raises an error\n",
      "     |  \n",
      "     |  shrink_threshold : float, default=None\n",
      "     |      Threshold for shrinking centroids to remove features.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  centroids_ : array-like of shape (n_classes, n_features)\n",
      "     |      Centroid of each class.\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.neighbors import NearestCentroid\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> clf = NearestCentroid()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  NearestCentroid()\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier : Nearest neighbors classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When used for text classification with tf-idf vectors, this classifier is\n",
      "     |  also known as the Rocchio classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of\n",
      "     |  multiple cancer types by shrunken centroids of gene expression. Proceedings\n",
      "     |  of the National Academy of Sciences of the United States of America,\n",
      "     |  99(10), 6567-6572. The National Academy of Sciences.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NearestCentroid\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, metric='euclidean', *, shrink_threshold=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the NearestCentroid model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |          Note that centroid shrinking cannot be used with sparse matrices.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      The predicted class C for each sample in X is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : ndarray of shape (n_samples,)\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If the metric constructor parameter is \"precomputed\", X is assumed to\n",
      "     |      be the distance matrix between the data to be predicted and\n",
      "     |      ``self.centroids_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NearestNeighbors(sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Unsupervised learner for implementing neighbor searches.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_neighbors : int, default=5\n",
      "     |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      "     |  \n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      the distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str\n",
      "     |      Metric used to compute distances to neighbors.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Parameters for the metric used to compute distances to neighbors.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |  >>> samples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n",
      "     |  \n",
      "     |  >>> neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
      "     |  >>> neigh.fit(samples)\n",
      "     |  NearestNeighbors(...)\n",
      "     |  \n",
      "     |  >>> neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False)\n",
      "     |  array([[2, 0]]...)\n",
      "     |  \n",
      "     |  >>> nbrs = neigh.radius_neighbors(\n",
      "     |  ...    [[0, 0, 1.3]], 0.4, return_distance=False\n",
      "     |  ... )\n",
      "     |  >>> np.asarray(nbrs[0][0])\n",
      "     |  array(2)\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier\n",
      "     |  RadiusNeighborsClassifier\n",
      "     |  KNeighborsRegressor\n",
      "     |  RadiusNeighborsRegressor\n",
      "     |  BallTree\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NearestNeighbors\n",
      "     |      sklearn.neighbors._base.KNeighborsMixin\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the nearest neighbors estimator from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : NearestNeighbors\n",
      "     |          The fitted nearest neighbors estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      "     |      Finds the K-neighbors of a point.\n",
      "     |      \n",
      "     |      Returns indices of and distances to the neighbors of each point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors required for each sample. The default is the\n",
      "     |          value passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Array representing the lengths to points, only present if\n",
      "     |          return_distance=True\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      "     |          Indices of the nearest points in the population matrix.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NearestNeighbors\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1,1,1]\n",
      "     |      \n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(n_neighbors=1)\n",
      "     |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      "     |      (array([[0.5]]), array([[2]]))\n",
      "     |      \n",
      "     |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      "     |      element is at distance 0.5 and is the third element of samples\n",
      "     |      (indexes start at 0). You can also query for multiple points:\n",
      "     |      \n",
      "     |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      "     |      >>> neigh.kneighbors(X, return_distance=False)\n",
      "     |      array([[1],\n",
      "     |             [2]]...)\n",
      "     |  \n",
      "     |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      "     |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |          For ``metric='precomputed'`` the shape should be\n",
      "     |          (n_queries, n_indexed). Otherwise the shape should be\n",
      "     |          (n_queries, n_features).\n",
      "     |      \n",
      "     |      n_neighbors : int, default=None\n",
      "     |          Number of neighbors for each sample. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix is of CSR format.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(n_neighbors=2)\n",
      "     |      >>> A = neigh.kneighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 1.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      NearestNeighbors.radius_neighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Finds the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix if of format CSR.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class NeighborhoodComponentsAnalysis(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  Neighborhood Components Analysis\n",
      "     |  \n",
      "     |  Neighborhood Component Analysis (NCA) is a machine learning algorithm for\n",
      "     |  metric learning. It learns a linear transformation in a supervised fashion\n",
      "     |  to improve the classification accuracy of a stochastic nearest neighbors\n",
      "     |  rule in the transformed space.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <nca>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int, default=None\n",
      "     |      Preferred dimensionality of the projected space.\n",
      "     |      If None it will be set to ``n_features``.\n",
      "     |  \n",
      "     |  init : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto'\n",
      "     |      Initialization of the linear transformation. Possible options are\n",
      "     |      'auto', 'pca', 'lda', 'identity', 'random', and a numpy array of shape\n",
      "     |      (n_features_a, n_features_b).\n",
      "     |  \n",
      "     |      'auto'\n",
      "     |          Depending on ``n_components``, the most reasonable initialization\n",
      "     |          will be chosen. If ``n_components <= n_classes`` we use 'lda', as\n",
      "     |          it uses labels information. If not, but\n",
      "     |          ``n_components < min(n_features, n_samples)``, we use 'pca', as\n",
      "     |          it projects data in meaningful directions (those of higher\n",
      "     |          variance). Otherwise, we just use 'identity'.\n",
      "     |  \n",
      "     |      'pca'\n",
      "     |          ``n_components`` principal components of the inputs passed\n",
      "     |          to :meth:`fit` will be used to initialize the transformation.\n",
      "     |          (See :class:`~sklearn.decomposition.PCA`)\n",
      "     |  \n",
      "     |      'lda'\n",
      "     |          ``min(n_components, n_classes)`` most discriminative\n",
      "     |          components of the inputs passed to :meth:`fit` will be used to\n",
      "     |          initialize the transformation. (If ``n_components > n_classes``,\n",
      "     |          the rest of the components will be zero.) (See\n",
      "     |          :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\n",
      "     |  \n",
      "     |      'identity'\n",
      "     |          If ``n_components`` is strictly smaller than the\n",
      "     |          dimensionality of the inputs passed to :meth:`fit`, the identity\n",
      "     |          matrix will be truncated to the first ``n_components`` rows.\n",
      "     |  \n",
      "     |      'random'\n",
      "     |          The initial transformation will be a random array of shape\n",
      "     |          `(n_components, n_features)`. Each value is sampled from the\n",
      "     |          standard normal distribution.\n",
      "     |  \n",
      "     |      numpy array\n",
      "     |          n_features_b must match the dimensionality of the inputs passed to\n",
      "     |          :meth:`fit` and n_features_a must be less than or equal to that.\n",
      "     |          If ``n_components`` is not None, n_features_a must match it.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If True and :meth:`fit` has been called before, the solution of the\n",
      "     |      previous call to :meth:`fit` is used as the initial linear\n",
      "     |      transformation (``n_components`` and ``init`` will be ignored).\n",
      "     |  \n",
      "     |  max_iter : int, default=50\n",
      "     |      Maximum number of iterations in the optimization.\n",
      "     |  \n",
      "     |  tol : float, default=1e-5\n",
      "     |      Convergence tolerance for the optimization.\n",
      "     |  \n",
      "     |  callback : callable, default=None\n",
      "     |      If not None, this function is called after every iteration of the\n",
      "     |      optimizer, taking as arguments the current solution (flattened\n",
      "     |      transformation matrix) and the number of iterations. This might be\n",
      "     |      useful in case one wants to examine or store the transformation\n",
      "     |      found after each iteration.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      If 0, no progress messages will be printed.\n",
      "     |      If 1, progress messages will be printed to stdout.\n",
      "     |      If > 1, progress messages will be printed and the ``disp``\n",
      "     |      parameter of :func:`scipy.optimize.minimize` will be set to\n",
      "     |      ``verbose - 2``.\n",
      "     |  \n",
      "     |  random_state : int or numpy.RandomState, default=None\n",
      "     |      A pseudo random number generator object or a seed for it if int. If\n",
      "     |      ``init='random'``, ``random_state`` is used to initialize the random\n",
      "     |      transformation. If ``init='pca'``, ``random_state`` is passed as an\n",
      "     |      argument to PCA when initializing the transformation. Pass an int\n",
      "     |      for reproducible results across multiple function calls.\n",
      "     |      See :term: `Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  components_ : ndarray of shape (n_components, n_features)\n",
      "     |      The linear transformation learned during fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Counts the number of iterations performed by the optimizer.\n",
      "     |  \n",
      "     |  random_state_ : numpy.RandomState\n",
      "     |      Pseudo random number generator object used during initialization.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
      "     |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ... stratify=y, test_size=0.7, random_state=42)\n",
      "     |  >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
      "     |  >>> nca.fit(X_train, y_train)\n",
      "     |  NeighborhoodComponentsAnalysis(...)\n",
      "     |  >>> knn = KNeighborsClassifier(n_neighbors=3)\n",
      "     |  >>> knn.fit(X_train, y_train)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(knn.score(X_test, y_test))\n",
      "     |  0.933333...\n",
      "     |  >>> knn.fit(nca.transform(X_train), y_train)\n",
      "     |  KNeighborsClassifier(...)\n",
      "     |  >>> print(knn.score(nca.transform(X_test), y_test))\n",
      "     |  0.961904...\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.\n",
      "     |         \"Neighbourhood Components Analysis\". Advances in Neural Information\n",
      "     |         Processing Systems. 17, 513-520, 2005.\n",
      "     |         http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf\n",
      "     |  \n",
      "     |  .. [2] Wikipedia entry on Neighborhood Components Analysis\n",
      "     |         https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NeighborhoodComponentsAnalysis\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=None, *, init='auto', warm_start=False, max_iter=50, tol=1e-05, callback=None, verbose=0, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The corresponding training labels.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns a trained NeighborhoodComponentsAnalysis model.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Applies the learned transformation to the given data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Data samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_embedded: ndarray of shape (n_samples, n_components)\n",
      "     |          The data samples transformed.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      NotFittedError\n",
      "     |          If :meth:`fit` has not been called before.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsClassifier(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Classifier implementing a vote among neighbors within a given radius\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      the distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  outlier_label : {manual label, 'most_frequent'}, default=None\n",
      "     |      label for outlier samples (samples with no neighbors in given radius).\n",
      "     |  \n",
      "     |      - manual label: str or int label (should be the same type as y)\n",
      "     |        or list of manual labels if multi-output is used.\n",
      "     |      - 'most_frequent' : assign the most frequent label of y to outliers.\n",
      "     |      - None : when any outlier is detected, ValueError will be raised.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels known to the classifier.\n",
      "     |  \n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  outlier_label_ : int or array-like of shape (n_class,)\n",
      "     |      Label which is given for outlier samples (samples with no neighbors\n",
      "     |      on given radius).\n",
      "     |  \n",
      "     |  outputs_2d_ : bool\n",
      "     |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      "     |      otherwise True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsClassifier\n",
      "     |  >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  RadiusNeighborsClassifier(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0]\n",
      "     |  >>> print(neigh.predict_proba([[1.0]]))\n",
      "     |  [[0.66666667 0.33333333]]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  KNeighborsClassifier\n",
      "     |  RadiusNeighborsRegressor\n",
      "     |  KNeighborsRegressor\n",
      "     |  NearestNeighbors\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsClassifier\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the radius neighbors classifier from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsClassifier\n",
      "     |          The fitted radius neighbors classifier.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the class labels for the provided data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      "     |          Class labels for each data sample.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test data X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n",
      "     |          of such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. Classes are ordered\n",
      "     |          by lexicographic order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Finds the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix if of format CSR.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsRegressor(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Regression based on neighbors within a fixed radius.\n",
      "     |  \n",
      "     |  The target is predicted by local interpolation of the targets\n",
      "     |  associated of the nearest neighbors in the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.9\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  radius : float, default=1.0\n",
      "     |      Range of parameter space to use by default for :meth:`radius_neighbors`\n",
      "     |      queries.\n",
      "     |  \n",
      "     |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      "     |      weight function used in prediction.  Possible values:\n",
      "     |  \n",
      "     |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      "     |        are weighted equally.\n",
      "     |      - 'distance' : weight points by the inverse of their distance.\n",
      "     |        in this case, closer neighbors of a query point will have a\n",
      "     |        greater influence than neighbors which are further away.\n",
      "     |      - [callable] : a user-defined function which accepts an\n",
      "     |        array of distances, and returns an array of the same shape\n",
      "     |        containing the weights.\n",
      "     |  \n",
      "     |      Uniform weights are used by default.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      the distance metric to use for the tree.  The default metric is\n",
      "     |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      "     |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      "     |      list of available metrics.\n",
      "     |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      "     |      must be square during fit. X may be a :term:`sparse graph`,\n",
      "     |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric to use. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> X = [[0], [1], [2], [3]]\n",
      "     |  >>> y = [0, 0, 1, 1]\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsRegressor\n",
      "     |  >>> neigh = RadiusNeighborsRegressor(radius=1.0)\n",
      "     |  >>> neigh.fit(X, y)\n",
      "     |  RadiusNeighborsRegressor(...)\n",
      "     |  >>> print(neigh.predict([[1.5]]))\n",
      "     |  [0.5]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  NearestNeighbors\n",
      "     |  KNeighborsRegressor\n",
      "     |  KNeighborsClassifier\n",
      "     |  RadiusNeighborsClassifier\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      "     |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsRegressor\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the radius neighbors regressor from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsRegressor\n",
      "     |          The fitted radius neighbors regressor.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target for the provided data\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double\n",
      "     |          Target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Finds the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix if of format CSR.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RadiusNeighborsTransformer(sklearn.neighbors._base.RadiusNeighborsMixin, sklearn.base.TransformerMixin, sklearn.neighbors._base.NeighborsBase)\n",
      "     |  Transform X into a (weighted) graph of neighbors nearer than a radius\n",
      "     |  \n",
      "     |  The transformed data is a sparse graph as returned by\n",
      "     |  radius_neighbors_graph.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <neighbors_transformer>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  mode : {'distance', 'connectivity'}, default='distance'\n",
      "     |      Type of returned matrix: 'connectivity' will return the connectivity\n",
      "     |      matrix with ones and zeros, and 'distance' will return the distances\n",
      "     |      between neighbors according to the given metric.\n",
      "     |  \n",
      "     |  radius : float, default=1.\n",
      "     |      Radius of neighborhood in the transformed sparse graph.\n",
      "     |  \n",
      "     |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      "     |      Algorithm used to compute the nearest neighbors:\n",
      "     |  \n",
      "     |      - 'ball_tree' will use :class:`BallTree`\n",
      "     |      - 'kd_tree' will use :class:`KDTree`\n",
      "     |      - 'brute' will use a brute-force search.\n",
      "     |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      "     |        based on the values passed to :meth:`fit` method.\n",
      "     |  \n",
      "     |      Note: fitting on sparse input will override the setting of\n",
      "     |      this parameter, using brute force.\n",
      "     |  \n",
      "     |  leaf_size : int, default=30\n",
      "     |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      "     |      speed of the construction and query, as well as the memory\n",
      "     |      required to store the tree.  The optimal value depends on the\n",
      "     |      nature of the problem.\n",
      "     |  \n",
      "     |  metric : str or callable, default='minkowski'\n",
      "     |      metric to use for distance computation. Any metric from scikit-learn\n",
      "     |      or scipy.spatial.distance can be used.\n",
      "     |  \n",
      "     |      If metric is a callable function, it is called on each\n",
      "     |      pair of instances (rows) and the resulting value recorded. The callable\n",
      "     |      should take two arrays as input and return one value indicating the\n",
      "     |      distance between them. This works for Scipy's metrics, but is less\n",
      "     |      efficient than passing the metric name as a string.\n",
      "     |  \n",
      "     |      Distance matrices are not supported.\n",
      "     |  \n",
      "     |      Valid values for metric are:\n",
      "     |  \n",
      "     |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "     |        'manhattan']\n",
      "     |  \n",
      "     |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "     |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "     |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "     |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "     |        'yule']\n",
      "     |  \n",
      "     |      See the documentation for scipy.spatial.distance for details on these\n",
      "     |      metrics.\n",
      "     |  \n",
      "     |  p : int, default=2\n",
      "     |      Parameter for the Minkowski metric from\n",
      "     |      sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
      "     |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "     |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "     |  \n",
      "     |  metric_params : dict, default=None\n",
      "     |      Additional keyword arguments for the metric function.\n",
      "     |  \n",
      "     |  n_jobs : int, default=1\n",
      "     |      The number of parallel jobs to run for neighbors search.\n",
      "     |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  effective_metric_ : str or callable\n",
      "     |      The distance metric used. It will be same as the `metric` parameter\n",
      "     |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      "     |      'minkowski' and `p` parameter set to 2.\n",
      "     |  \n",
      "     |  effective_metric_params_ : dict\n",
      "     |      Additional keyword arguments for the metric function. For most metrics\n",
      "     |      will be same with `metric_params` parameter, but may also contain the\n",
      "     |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      "     |      'minkowski'.\n",
      "     |  \n",
      "     |  n_samples_fit_ : int\n",
      "     |      Number of samples in the fitted data.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.cluster import DBSCAN\n",
      "     |  >>> from sklearn.neighbors import RadiusNeighborsTransformer\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> estimator = make_pipeline(\n",
      "     |  ...     RadiusNeighborsTransformer(radius=42.0, mode='distance'),\n",
      "     |  ...     DBSCAN(min_samples=30, metric='precomputed'))\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RadiusNeighborsTransformer\n",
      "     |      sklearn.neighbors._base.RadiusNeighborsMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.neighbors._base.NeighborsBase\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, mode='distance', radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Fit the radius neighbors transformer from the training dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : RadiusNeighborsTransformer\n",
      "     |          The fitted radius neighbors transformer.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : ignored\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples, n_samples)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples_transform, n_features)\n",
      "     |          Sample data\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : sparse matrix of shape (n_samples_transform, n_samples_fit)\n",
      "     |          Xt[i, j] is assigned the weight of edge that connects i to j.\n",
      "     |          Only the neighbors have an explicit value.\n",
      "     |          The diagonal is always explicit.\n",
      "     |          The matrix is of CSR format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  radius_neighbors(self, X=None, radius=None, return_distance=True, sort_results=False)\n",
      "     |      Finds the neighbors within a given radius of a point or points.\n",
      "     |      \n",
      "     |      Return the indices and distances of each point from the dataset\n",
      "     |      lying in a ball with size ``radius`` around the points of the query\n",
      "     |      array. Points lying on the boundary are included in the results.\n",
      "     |      \n",
      "     |      The result points are *not* necessarily sorted by distance to their\n",
      "     |      query point.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Limiting distance of neighbors to return. The default is the value\n",
      "     |          passed to the constructor.\n",
      "     |      \n",
      "     |      return_distance : bool, default=True\n",
      "     |          Whether or not to return the distances.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, the distances and indices will be sorted by increasing\n",
      "     |          distances before being returned. If False, the results may not\n",
      "     |          be sorted. If `return_distance=False`, setting `sort_results=True`\n",
      "     |          will result in an error.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      neigh_dist : ndarray of shape (n_samples,) of arrays\n",
      "     |          Array representing the distances to each point, only present if\n",
      "     |          `return_distance=True`. The distance values are computed according\n",
      "     |          to the ``metric`` constructor parameter.\n",
      "     |      \n",
      "     |      neigh_ind : ndarray of shape (n_samples,) of arrays\n",
      "     |          An array of arrays of indices of the approximate nearest points\n",
      "     |          from the population matrix that lie within a ball of size\n",
      "     |          ``radius`` around the query points.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      In the following example, we construct a NeighborsClassifier\n",
      "     |      class from an array representing our data set and ask who's\n",
      "     |      the closest point to [1, 1, 1]:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.6)\n",
      "     |      >>> neigh.fit(samples)\n",
      "     |      NearestNeighbors(radius=1.6)\n",
      "     |      >>> rng = neigh.radius_neighbors([[1., 1., 1.]])\n",
      "     |      >>> print(np.asarray(rng[0][0]))\n",
      "     |      [1.5 0.5]\n",
      "     |      >>> print(np.asarray(rng[1][0]))\n",
      "     |      [1 2]\n",
      "     |      \n",
      "     |      The first array returned contains the distances to all points which\n",
      "     |      are closer than 1.6, while the second array returned contains their\n",
      "     |      indices.  In general, multiple points can be queried at the same time.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Because the number of neighbors of each point is not necessarily\n",
      "     |      equal, the results for multiple query points cannot be fit in a\n",
      "     |      standard data array.\n",
      "     |      For efficiency, `radius_neighbors` returns arrays of objects, where\n",
      "     |      each object is a 1D array of indices or distances.\n",
      "     |  \n",
      "     |  radius_neighbors_graph(self, X=None, radius=None, mode='connectivity', sort_results=False)\n",
      "     |      Computes the (weighted) graph of Neighbors for points in X\n",
      "     |      \n",
      "     |      Neighborhoods are restricted the points at a distance lower than\n",
      "     |      radius.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features), default=None\n",
      "     |          The query point or points.\n",
      "     |          If not provided, neighbors of each indexed point are returned.\n",
      "     |          In this case, the query point is not considered its own neighbor.\n",
      "     |      \n",
      "     |      radius : float, default=None\n",
      "     |          Radius of neighborhoods. The default is the value passed to the\n",
      "     |          constructor.\n",
      "     |      \n",
      "     |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "     |          Type of returned matrix: 'connectivity' will return the\n",
      "     |          connectivity matrix with ones and zeros, in 'distance' the\n",
      "     |          edges are Euclidean distance between points.\n",
      "     |      \n",
      "     |      sort_results : bool, default=False\n",
      "     |          If True, in each row of the result, the non-zero entries will be\n",
      "     |          sorted by increasing distances. If False, the non-zero entries may\n",
      "     |          not be sorted. Only used with mode='distance'.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.22\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      "     |          `n_samples_fit` is the number of samples in the fitted data\n",
      "     |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      "     |          The matrix if of format CSR.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> X = [[0], [3], [1]]\n",
      "     |      >>> from sklearn.neighbors import NearestNeighbors\n",
      "     |      >>> neigh = NearestNeighbors(radius=1.5)\n",
      "     |      >>> neigh.fit(X)\n",
      "     |      NearestNeighbors(radius=1.5)\n",
      "     |      >>> A = neigh.radius_neighbors_graph(X)\n",
      "     |      >>> A.toarray()\n",
      "     |      array([[1., 0., 1.],\n",
      "     |             [0., 1., 0.],\n",
      "     |             [1., 0., 1.]])\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      kneighbors_graph\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.neighbors._base.RadiusNeighborsMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    kneighbors_graph(X, n_neighbors, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\n",
      "        Computes the (weighted) graph of k-Neighbors for points in X\n",
      "        \n",
      "        Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features) or BallTree\n",
      "            Sample data, in the form of a numpy array or a precomputed\n",
      "            :class:`BallTree`.\n",
      "        \n",
      "        n_neighbors : int\n",
      "            Number of neighbors for each sample.\n",
      "        \n",
      "        mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "            Type of returned matrix: 'connectivity' will return the connectivity\n",
      "            matrix with ones and zeros, and 'distance' will return the distances\n",
      "            between neighbors according to the given metric.\n",
      "        \n",
      "        metric : str, default='minkowski'\n",
      "            The distance metric used to calculate the k-Neighbors for each sample\n",
      "            point. The DistanceMetric class gives a list of available metrics.\n",
      "            The default distance is 'euclidean' ('minkowski' metric with the p\n",
      "            param equal to 2.)\n",
      "        \n",
      "        p : int, default=2\n",
      "            Power parameter for the Minkowski metric. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            additional keyword arguments for the metric function.\n",
      "        \n",
      "        include_self : bool or 'auto', default=False\n",
      "            Whether or not to mark each sample as the first nearest neighbor to\n",
      "            itself. If 'auto', then True is used for mode='connectivity' and False\n",
      "            for mode='distance'.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A : sparse matrix of shape (n_samples, n_samples)\n",
      "            Graph where A[i, j] is assigned the weight of edge that\n",
      "            connects i to j. The matrix is of CSR format.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> X = [[0], [3], [1]]\n",
      "        >>> from sklearn.neighbors import kneighbors_graph\n",
      "        >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n",
      "        >>> A.toarray()\n",
      "        array([[1., 0., 1.],\n",
      "               [0., 1., 1.],\n",
      "               [1., 0., 1.]])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        radius_neighbors_graph\n",
      "    \n",
      "    radius_neighbors_graph(X, radius, *, mode='connectivity', metric='minkowski', p=2, metric_params=None, include_self=False, n_jobs=None)\n",
      "        Computes the (weighted) graph of Neighbors for points in X\n",
      "        \n",
      "        Neighborhoods are restricted the points at a distance lower than\n",
      "        radius.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features) or BallTree\n",
      "            Sample data, in the form of a numpy array or a precomputed\n",
      "            :class:`BallTree`.\n",
      "        \n",
      "        radius : float\n",
      "            Radius of neighborhoods.\n",
      "        \n",
      "        mode : {'connectivity', 'distance'}, default='connectivity'\n",
      "            Type of returned matrix: 'connectivity' will return the connectivity\n",
      "            matrix with ones and zeros, and 'distance' will return the distances\n",
      "            between neighbors according to the given metric.\n",
      "        \n",
      "        metric : str, default='minkowski'\n",
      "            The distance metric used to calculate the neighbors within a\n",
      "            given radius for each sample point. The DistanceMetric class\n",
      "            gives a list of available metrics. The default distance is\n",
      "            'euclidean' ('minkowski' metric with the param equal to 2.)\n",
      "        \n",
      "        p : int, default=2\n",
      "            Power parameter for the Minkowski metric. When p = 1, this is\n",
      "            equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      "            (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      "        \n",
      "        metric_params : dict, default=None\n",
      "            additional keyword arguments for the metric function.\n",
      "        \n",
      "        include_self : bool or 'auto', default=False\n",
      "            Whether or not to mark each sample as the first nearest neighbor to\n",
      "            itself. If 'auto', then True is used for mode='connectivity' and False\n",
      "            for mode='distance'.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of parallel jobs to run for neighbors search.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A : sparse matrix of shape (n_samples, n_samples)\n",
      "            Graph where A[i, j] is assigned the weight of edge that connects\n",
      "            i to j. The matrix is of CSR format.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> X = [[0], [3], [1]]\n",
      "        >>> from sklearn.neighbors import radius_neighbors_graph\n",
      "        >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity',\n",
      "        ...                            include_self=True)\n",
      "        >>> A.toarray()\n",
      "        array([[1., 0., 1.],\n",
      "               [0., 1., 0.],\n",
      "               [1., 0., 1.]])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kneighbors_graph\n",
      "\n",
      "DATA\n",
      "    VALID_METRICS = {'ball_tree': ['euclidean', 'l2', 'minkowski', 'p', 'm...\n",
      "    VALID_METRICS_SPARSE = {'ball_tree': [], 'brute': {'cityblock', 'cosin...\n",
      "    __all__ = ['BallTree', 'DistanceMetric', 'KDTree', 'KNeighborsClassifi...\n",
      "\n",
      "FILE\n",
      "    c:\\bda-test\\venv\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "help(sklearn.neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce55ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_confusion_matrix` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  plot_confusion_matrix : Plot Confusion Matrix.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot() # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetCurveDisplay(builtins.object)\n",
      "     |  DET curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_det_curve` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  plot_det_curve : Plot detection error tradeoff (DET) curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, fnr, thresholds = metrics.det_curve(y, pred)\n",
      "     |  >>> display = metrics.DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name='example estimator'\n",
      "     |  ... )\n",
      "     |  >>> display.plot()  # doctest: +SKIP\n",
      "     |  >>> plt.show()      # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PrecisionRecallDisplay(builtins.object)\n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_precision_recall_curve`\n",
      "     |  to create a visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  -----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  plot_precision_recall_curve : Plot Precision Recall Curve for binary\n",
      "     |      classifiers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot() # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use the\n",
      "     |          name of the estimator.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RocCurveDisplay(builtins.object)\n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_roc_curve` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  roc_auc : float, default=None\n",
      "     |      Area under ROC curve. If None, the roc_auc score is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      ROC Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,                                          estimator_name='example estimator')\n",
      "     |  >>> display.plot()  # doctest: +SKIP\n",
      "     |  >>> plt.show()      # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jaccard_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray of shape (n,)\n",
      "            x coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : ndarray of shape, (n,)\n",
      "            y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83...\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, and perfect performance scores 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        recall_score, roc_auc_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). It can be decomposed is the sum of refinement loss and\n",
      "        calibration loss.\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter `pos_label`, which defaults to\n",
      "        the greater label unless `y_true` is all 0 or all -1, in which case\n",
      "        `pos_label` defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class. `pos_label` will be infered in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitely specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, *, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of str of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string / dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_confusion_matrix : Plot Confusion Matrix.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Computes the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If None, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None)\n",
      "        Compute error rates for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance propability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing score values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_det_curve : Plot detection error tradeoff (DET) curve.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples,), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as ``float32``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances betweens pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true, y_pred, zero_division=1)\n",
      "        1.0...\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> +inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str or callable\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {None, 'micro', 'macro', 'samples', 'weighted',             'binary'}, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, f_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        and a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float, default=1e-15\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "    \n",
      "    make_scorer(score_func, *, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in\n",
      "        :class:`~sklearn.model_selection.GridSearchCV` and\n",
      "        :func:`~sklearn.model_selection.cross_val_score`.\n",
      "        It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`,\n",
      "        :func:`~sklearn.metrics.mean_squared_error`,\n",
      "        :func:`~sklearn.metrics.adjusted_rand_index` or\n",
      "        :func:`~sklearn.metrics.average_precision`\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        The signature of the call is `(estimator, X, y)` where `estimator`\n",
      "        is the model to be evaluated, `X` is the data and `y` is the\n",
      "        ground truth labeling (or `None` in the case of unsupervised models).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : bool, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "        needs_threshold : bool, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If `needs_proba=False` and `needs_threshold=False`, the score\n",
      "        function is supposed to accept the output of :term:`predict`. If\n",
      "        `needs_proba=True`, the score function is supposed to accept the\n",
      "        output of :term:`predict_proba` (For binary `y_true`, the score function is\n",
      "        supposed to accept probability of the positive class). If\n",
      "        `needs_threshold=True`, the score function is supposed to accept the\n",
      "        output of :term:`decision_function`.\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error regression loss.\n",
      "        \n",
      "        Note here that we do not represent the output as a percentage in range\n",
      "        [0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the\n",
      "        :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats in the range [0, 1/eps]\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note the fact that bad predictions can lead to arbitarily large\n",
      "            MAPE values, especially if some y_true values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when y_true is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSE value, if False returns RMSE value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.822...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while confusion_matrix calculates\n",
      "        one confusion matrix for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels of\n",
      "        the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        contingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape=(n_samples_X, n_features)\n",
      "        \n",
      "        Y : array-like of shape=(n_samples_Y, n_features), default=None\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If None, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69...\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35...\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rand_score: Rand Score\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding elements\n",
      "            of two arrays.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n",
      "            None is useful for in-place operations, rather than reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    plot_confusion_matrix(estimator, X, y_true, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True)\n",
      "        Plot Confusion Matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder or\n",
      "            select a subset of labels. If `None` is given, those that appear at\n",
      "            least once in `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        display_labels : array-like of shape (n_classes,), default=None\n",
      "            Target names used for plotting. By default, `labels` will be used if\n",
      "            it is defined, otherwise the unique labels of `y_true` and `y_pred`\n",
      "            will be used.\n",
      "        \n",
      "        include_values : bool, default=True\n",
      "            Includes values in confusion matrix.\n",
      "        \n",
      "        xticks_rotation : {'vertical', 'horizontal'} or float,                         default='horizontal'\n",
      "            Rotation of xtick labels.\n",
      "        \n",
      "        values_format : str, default=None\n",
      "            Format specification for values in confusion matrix. If `None`,\n",
      "            the format specification is 'd' or '.2g' whichever is shorter.\n",
      "        \n",
      "        cmap : str or matplotlib Colormap, default='viridis'\n",
      "            Colormap recognized by matplotlib.\n",
      "        \n",
      "        ax : matplotlib Axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is\n",
      "            created.\n",
      "        \n",
      "        colorbar : bool, default=True\n",
      "            Whether or not to add a colorbar to the plot.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.metrics import plot_confusion_matrix\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> from sklearn.svm import SVC\n",
      "        >>> X, y = make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...         X, y, random_state=0)\n",
      "        >>> clf = SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> plot_confusion_matrix(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()  # doctest: +SKIP\n",
      "    \n",
      "    plot_det_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot detection error tradeoff (DET) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}             default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the predicted target response. If set to\n",
      "            'auto', :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of DET curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The label of the positive class.\n",
      "            When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            `pos_label` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        det_curve : Compute error rates for different probability thresholds.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn import datasets, metrics, model_selection, svm\n",
      "        >>> X, y = datasets.make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
      "        ...     X, y, random_state=0)\n",
      "        >>> clf = svm.SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> metrics.plot_det_curve(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()                                   # doctest: +SKIP\n",
      "    \n",
      "    plot_precision_recall_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot Precision Recall Curve for binary classifiers.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Binary target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'},                       default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name for labeling curve. If `None`, the name of the\n",
      "            estimator is used.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the precision\n",
      "            and recall metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        **kwargs : dict\n",
      "            Keyword arguments to be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        PrecisionRecallDisplay : Precision Recall visualization.\n",
      "    \n",
      "    plot_roc_curve(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : boolean, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}     default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the roc auc\n",
      "            metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay : ROC Curve visualization.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn import datasets, metrics, model_selection, svm\n",
      "        >>> X, y = datasets.make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
      "        ...     X, y, random_state=0)\n",
      "        >>> clf = svm.SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()                                   # doctest: +SKIP\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, *, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : ndarray of shape (n_samples,)\n",
      "            Estimated probabilities or output of a decision function.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall. n_thresholds <= len(np.unique(probas_pred)).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_precision_recall_curve : Plot Precision Recall Curve for binary\n",
      "            classifiers.\n",
      "        PrecisionRecallDisplay : Precision Recall visualization.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'binary', 'micro', 'macro', 'samples','weighted'},             default=None\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "               - recall: when there are no positive labels\n",
      "               - precision: when there are no positive predictions\n",
      "               - f-score: both\n",
      "        \n",
      "            If set to \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape\n",
      "            (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33..., 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33..., 1.        , 1.        ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        R^2 (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a R^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The R^2 score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, R^2 score may be negative (it need not actually\n",
      "        be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is:\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Simple_matching_coefficient\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Rand_index\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape\n",
      "            (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, balanced_accuracy_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
      "        from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99...\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99...\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99...\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_pred = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      "        >>> roc_auc_score(y, y_pred, average=None)\n",
      "        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape = (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay : ROC Curve visualization.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers). The binary case expects\n",
      "            scores with shape (n_samples,) while the multiclass case expects scores\n",
      "            with shape (n_samples, n_classes). In the nulticlass case, the order of\n",
      "            the class scores must correspond to the order of ``labels``, if\n",
      "            provided, or else to the numerical or lexicographical order of the\n",
      "            labels in ``y_true``.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        normalized_mutual_info_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    c:\\bda-test\\venv\\lib\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.metrics )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8997229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
